{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TestingPipeline.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"ER61MXF5tY93","executionInfo":{"status":"ok","timestamp":1601424663995,"user_tz":300,"elapsed":45449,"user":{"displayName":"Dushyant Singh Khinchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhVgT4J-0PZxTmXsjDZlk4iTD5UX35a5leV156K=s64","userId":"14979082350769729724"}},"outputId":"8417b14b-bd78-48b2-c035-fe07bf275188","colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["# authorize google account to use bigquery\n","from google.colab import auth\n","auth.authenticate_user()\n","print('Authenticated')\n","\n","# authorize drive to pull model\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# pull data for repo from bigquery\n","project_id = 'uic-capstone-int'\n","from google.cloud import bigquery\n","\n","client = bigquery.Client(project=project_id)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Authenticated\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AHQcou2tgmem"},"source":["from tensorflow import keras\n","import pandas as pd\n","import tensorflow as tf\n","from keras.models import Sequential\n","from keras.layers import LSTM\n","from keras.layers import Dense\n","import matplotlib.pyplot as plt\n","\n","\n","#Genesis model for weight initialization\n","model_withTransfer = keras.models.load_model('drive/Shared drives/IDS 560 - Fall 20/model101.h5')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7hMB4TteV-lO"},"source":["#Unique repo id's generated since 2011 (these id's are collected from bigquery to be used in a training loop that we'll be building around the pipeline)\n","unique_repo_df = pd.read_csv('drive/My Drive/unique_repoid/bq-results-20200929-031543-adwz8kt104m3.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QeGUjup2Wld2"},"source":["unique_repo_id = unique_repo_df['repoID'].tolist()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OzQT9m4BXJYK"},"source":["import random\n","unique_repo_id= random.sample(unique_repo_id, 1000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gvgZnidvqaat"},"source":["\n","\n","for i in range(len(unique_repo_id)):\n","\n","    id= unique_repo_id[i]\n","\n","\n","\n","\n","    # This cell is for data fetching process\n","\n","    #-----------------------Pushes---------------------------------------------------\n","\n","    sql = '''\n","    SELECT FORMAT_DATETIME(\"%Y-%m-%d\", DATETIME(date)) AS year_month, repoID, count (*) number_of_pushes\n","    FROM\n","    (\n","\n","    SELECT created_at as date, repo.id as repoID\n","    FROM githubarchive.year.2015\n","    WHERE type='PushEvent' AND repo.id = {0}\n","\n","    UNION ALL\n","\n","    SELECT created_at as date, repo.id as repoID\n","    FROM githubarchive.year.2016\n","    WHERE type='PushEvent' AND repo.id = {0}\n","\n","    UNION ALL\n","\n","    SELECT created_at as date, repo.id as repoID\n","    FROM githubarchive.year.2017\n","    WHERE type='PushEvent' AND repo.id = {0}\n","\n","    UNION ALL\n","\n","    SELECT created_at as date, repo.id as repoID\n","    FROM githubarchive.year.2018\n","    WHERE type='PushEvent' AND repo.id = {0}\n","\n","    UNION ALL\n","\n","    SELECT created_at as date, repo.id as repoID\n","    FROM githubarchive.year.2019\n","    WHERE type='PushEvent' AND repo.id = {0}\n","    )\n","    GROUP BY 1,2\n","    ORDER BY 1;'''.format(id)\n","    dfPushes=client.query(sql).to_dataframe()\n","\n","\n","\n","\n","    #---------------------------------Forks------------------------------------------\n","\n","    sql='''\n","\n","    SELECT FORMAT_DATETIME(\"%Y-%m-%d\", DATETIME(date)) AS year_month, repoID, count (*) number_of_forks\n","    FROM\n","    (\n","\n","    SELECT created_at as date, repo.id as repoID\n","    FROM githubarchive.year.2015\n","    WHERE type='ForkEvent' AND repo.id = {0}\n","\n","    UNION ALL\n","\n","    SELECT created_at as date, repo.id as repoID\n","    FROM githubarchive.year.2016\n","    WHERE type='ForkEvent' AND repo.id = {0}\n","\n","    UNION ALL\n","\n","    SELECT created_at as date, repo.id as repoID\n","    FROM githubarchive.year.2017\n","    WHERE type='ForkEvent' AND repo.id = {0}\n","\n","    UNION ALL\n","\n","    SELECT created_at as date, repo.id as repoID\n","    FROM githubarchive.year.2018\n","    WHERE type='ForkEvent' AND repo.id = {0}\n","\n","    UNION ALL\n","\n","    SELECT created_at as date, repo.id as repoID\n","    FROM githubarchive.year.2019\n","    WHERE type='ForkEvent' AND repo.id = {0}\n","    )\n","    GROUP BY 1,2\n","    ORDER BY 1;\n","    '''.format(id)\n","    dfForks=client.query(sql).to_dataframe()\n","\n","\n","\n","\n","    #-----------------------------------------Pulls------------------------------------\n","    sql = '''\n","    SELECT FORMAT_DATETIME(\"%Y-%m-%d\", DATETIME(date)) AS year_month, repoID, count (*) number_of_pulls\n","    FROM\n","    (\n","\n","    SELECT created_at as date, repo.id as repoID\n","    FROM githubarchive.year.2015\n","    WHERE type='PullRequestEvent' AND repo.id = {0}\n","\n","    UNION ALL\n","\n","    SELECT created_at as date, repo.id as repoID\n","    FROM githubarchive.year.2016\n","    WHERE type='PullRequestEvent' AND repo.id = {0}\n","\n","    UNION ALL\n","\n","    SELECT created_at as date, repo.id as repoID\n","    FROM githubarchive.year.2017\n","    WHERE type='PullRequestEvent' AND repo.id = {0}\n","\n","    UNION ALL\n","\n","    SELECT created_at as date, repo.id as repoID\n","    FROM githubarchive.year.2018\n","    WHERE type='PullRequestEvent' AND repo.id = {0}\n","\n","    UNION ALL\n","\n","    SELECT created_at as date, repo.id as repoID\n","    FROM githubarchive.year.2019\n","    WHERE type='PullRequestEvent' AND repo.id = {0}\n","    )\n","    GROUP BY 1,2\n","    ORDER BY 1;'''.format(id)\n","    dfPulls=client.query(sql).to_dataframe()\n","\n","\n","\n","    #---------------------------------------Bookmarks-------------------------------------\n","    sql= '''\n","\n","    SELECT FORMAT_DATETIME(\"%Y-%m-%d\", DATETIME(date)) AS year_month, repoID, count (DISTINCT actorID) number_of_Bookmarks\n","    FROM\n","    (\n","    SELECT created_at as date, repo.id as repoID, actor.id AS actorID\n","    FROM githubarchive.year.2015\n","    WHERE type='WatchEvent' AND repo.id = {0}\n","\n","    UNION ALL\n","\n","    SELECT created_at as date, repo.id as repoID, actor.id AS actorID\n","    FROM githubarchive.year.2016\n","    WHERE type='WatchEvent' AND repo.id = {0}\n","\n","    UNION ALL\n","\n","    SELECT created_at as date, repo.id as repoID, actor.id AS actorID\n","    FROM githubarchive.year.2017\n","    WHERE type='WatchEvent' AND repo.id = {0}\n","\n","    UNION ALL\n","\n","    SELECT created_at as date, repo.id as repoID, actor.id AS actorID\n","    FROM githubarchive.year.2018\n","    WHERE type='WatchEvent' AND repo.id = {0}\n","\n","    UNION ALL\n","\n","    SELECT created_at as date, repo.id as repoID, actor.id AS actorID\n","    FROM githubarchive.year.2019\n","    WHERE type='WatchEvent' AND repo.id = {0}\n","    )\n","    GROUP BY 1,2\n","    ORDER BY 1;'''.format(id)\n","    dfBoomarks = client.query(sql).to_dataframe()\n","\n","\n","\n","\n","\n","\n","    #----------------------------Issues---------------------------------------------\n","    sql = '''\n","    SELECT FORMAT_DATETIME(\"%Y-%m-%d\", DATETIME(date)) AS year_month, repoID, count (*) number_of_issues\n","    FROM\n","    (\n","\n","    SELECT created_at as date, repo.id as repoID\n","    FROM githubarchive.year.2015\n","    WHERE type='IssueEvent' AND repo.id = {0}\n","\n","    UNION ALL\n","\n","    SELECT created_at as date, repo.id as repoID\n","    FROM githubarchive.year.2016\n","    WHERE type='IssueEvent' AND repo.id = {0}\n","\n","    UNION ALL\n","\n","    SELECT created_at as date, repo.id as repoID\n","    FROM githubarchive.year.2017\n","    WHERE type='IssueEvent' AND repo.id = {0}\n","\n","    UNION ALL\n","\n","    SELECT created_at as date, repo.id as repoID\n","    FROM githubarchive.year.2018\n","    WHERE type='IssueEvent' AND repo.id = {0}\n","\n","    UNION ALL\n","\n","    SELECT created_at as date, repo.id as repoID\n","    FROM githubarchive.year.2019\n","    WHERE type='IssueEvent' AND repo.id = {0}\n","    )\n","    GROUP BY 1,2\n","    ORDER BY 1;'''.format(id)\n","    dfIssues=client.query(sql).to_dataframe()\n","\n","\n","\n","\n","\n","\n","    #-------------------------Issuecomments-------------------------------------------\n","\n","    sql = '''\n","    SELECT FORMAT_DATETIME(\"%Y-%m-%d\", DATETIME(date)) AS year_month, repoID, COUNT(*) AS issue_comment_count, COUNT( DISTINCT a_id) as issue_author_count\n","    FROM \n","    (SELECT created_at as date, repo.id as repoID, actor.id AS a_id\n","    FROM githubarchive.year.2015\n","    WHERE type = 'IssueCommentEvent' AND repo.id = {0}\n","\n","\n","    UNION ALL\n","\n","    SELECT created_at as date, repo.id as repoID, actor.id AS a_id\n","    FROM githubarchive.year.2016\n","    WHERE type = 'IssueCommentEvent' AND repo.id = {0}\n","\n","    UNION ALL\n","\n","\n","\n","    SELECT created_at as date, repo.id as repoID, actor.id AS a_id\n","    FROM githubarchive.year.2017\n","    WHERE type = 'IssueCommentEvent' AND repo.id = {0}\n","\n","    UNION ALL\n","\n","\n","    SELECT created_at as date, repo.id as repoID, actor.id AS a_id\n","    FROM githubarchive.year.2018\n","    WHERE type = 'IssueCommentEvent' AND repo.id = {0}\n","\n","    UNION ALL\n","\n","\n","    SELECT created_at as date, repo.id as repoID, actor.id AS a_id\n","    FROM githubarchive.year.2019\n","    WHERE type = 'IssueCommentEvent' AND repo.id = {0}\n","\n","    )\n","    GROUP BY 1,2\n","    ORDER BY 1;'''.format(id)\n","    dfIssuecomment = client.query(sql).to_dataframe()\n","\n","\n","\n","    #---------------------------------------Commitcomments------------------------------------------\n","    sql = '''\n","    SELECT FORMAT_DATETIME(\"%Y-%m-%d\", DATETIME(date)) AS year_month, repoID, COUNT(*) AS commit_comment_count, COUNT( DISTINCT a_id) AS commit_author_count\n","    FROM \n","    (SELECT created_at as date, repo.id as repoID, actor.id AS a_id\n","    FROM githubarchive.year.2015\n","    WHERE type = 'CommitCommentEvent' AND repo.id = {0}\n","\n","\n","    UNION ALL\n","\n","    SELECT created_at as date, repo.id as repoID, actor.id AS a_id\n","    FROM githubarchive.year.2016\n","    WHERE type = 'CommitCommentEvent' AND repo.id = {0}\n","\n","    UNION ALL\n","\n","\n","\n","    SELECT created_at as date, repo.id as repoID, actor.id AS a_id\n","    FROM githubarchive.year.2017\n","    WHERE type = 'CommitCommentEvent' AND repo.id = {0}\n","\n","    UNION ALL\n","\n","\n","    SELECT created_at as date, repo.id as repoID, actor.id AS a_id\n","    FROM githubarchive.year.2018\n","    WHERE type = 'CommitCommentEvent' AND repo.id = {0}\n","\n","    UNION ALL\n","\n","\n","    SELECT created_at as date, repo.id as repoID, actor.id AS a_id\n","    FROM githubarchive.year.2019\n","    WHERE type = 'CommitCommentEvent' AND repo.id = {0}\n","\n","    )\n","    GROUP BY 1,2\n","    ORDER BY 1;'''.format(id)\n","    dfCommitcomment = client.query(sql).to_dataframe()\n","\n","\n","\n","\n","\n","\n","    merge1 = pd.merge(dfPushes,dfForks,how='outer', on='year_month')\n","    merge2 = pd.merge(merge1, dfPulls, how='outer', on='year_month')\n","    merge3 = pd.merge(merge2, dfBoomarks, how='outer', on='year_month')\n","    merge4 = pd.merge(merge3, dfIssuecomment, how='outer', on='year_month')\n","    merge5 = pd.merge(merge4,dfCommitcomment,how='outer',on='year_month')\n","    merge6 = pd.merge(merge5, dfIssues, how='outer',on='year_month')\n","    merge6.drop(['repoID_x','repoID_y'], axis=1, inplace=True)\n","    merge6.drop('repoID', axis=1, inplace=True)\n","    merge6.sort_values(by='year_month',ascending=True,inplace=True)\n","    merge6\n","\n","\n","\n","\n","\n","    merge6['year_month']=pd.to_datetime(merge6['year_month'])\n","    merge6.rename({'year_month': 'datetime'}, axis=1, inplace=True)\n","    merge6.index=merge6['datetime']\n","    merge6.drop('datetime',axis=1,inplace=True)\n","    merge6\n","\n","\n","\n","\n","    #This step is for padding the dataframe to maintain the consistency of all the time series we'll be fetching in the future\n","    #This step also deals with the problem of missing dates(if no event happens at all) in the time series\n","    from datetime import datetime\n","    now=datetime.now()\n","    today = now.strftime(\"%Y-%m-%d\")\n","    today\n","\n","    idx = pd.date_range('2016-01-01', today)\n","    merge6.index = pd.DatetimeIndex(merge6.index)\n","\n","    merge6 = merge6.reindex(idx, fill_value=0)\n","\n","\n","    #aggregating the weekly data and creating weekly time steps\n","    WeeklyTS = pd.DataFrame()\n","    WeeklyTS = merge6.resample('W').sum()\n","\n","\n","    #Summing all the data to get an idea of the frequency of different events\n","    col_sum=WeeklyTS.sum(axis=0)\n","    col_sum.sort_values(ascending=False, inplace=True)\n","\n","\n","\n","\n","\n","\n","\n","    #dimensionality reduction just for the visualization\n","\n","    # from sklearn.decomposition import PCA\n","    # pca = PCA(n_components=2).fit(WeeklyTS)\n","    # pca_2d = pca.transform(WeeklyTS)\n","\n","\n","\n","\n","\n","    #clustering just to see any patterns and to create a dummy target variable\n","    from sklearn.cluster import KMeans\n","    Kmeansmodel = KMeans(n_clusters=2)\n","    Kmeansmodel.fit(WeeklyTS)\n","    y_kmeans = Kmeansmodel.predict(WeeklyTS)\n","\n","\n","\n","\n","    #Just adding a dummy target variable for testing\n","    WeeklyTS['status']=Kmeansmodel.labels_\n","    WeeklyTS['status'].value_counts()\n","\n","\n","\n","    #seperating date feature into 'year','month', and 'day' features\n","    WeeklyTS.insert(0, 'year', WeeklyTS.index.year)\n","    WeeklyTS.insert(1, 'month', WeeklyTS.index.month)\n","    WeeklyTS.insert(2, 'day', WeeklyTS.index.day)\n","    WeeklyTS.reset_index(level=0, inplace=True)\n","\n","\n","\n","    WeeklyTS.drop('index', axis=1, inplace=True)\n","    WeeklyTS\n","\n","\n","\n","    # yet to be performed - Normalization of the above dataset\n","\n","\n","\n","    # multivariate data preparation\n","\n","    from numpy import array\n","    from numpy import hstack\n","\n","    # split a multivariate sequence into samples\n","    def split_sequences(sequences, n_steps):\n","      X, y = list(), list()\n","      for i in range(len(sequences)):\n","        # end of the pattern\n","        end_ix = i + n_steps\n","        # check if surpaas the dataset\n","        if end_ix > len(sequences):\n","          break\n","        # input and output parts of the pattern\n","        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n","        X.append(seq_x)\n","        y.append(seq_y)\n","      return array(X), array(y)\n","    \n","\n","    # define input sequence\n","    t=[]\n","\n","    # transforming each column to an array\n","    for i in range(0,len(WeeklyTS.columns)):\n","      t.append(WeeklyTS.iloc[:,i].to_numpy())\n","\n","    # convert array to [rows, columns] structure\n","    for i in range(0,len(WeeklyTS.columns)):\n","      t[i]=t[i].reshape((len(t[i]),1))\n","\n","\n","\n","    # horizontally stack columns\n","    dataset=t[0]\n","    for i in range(1,len(WeeklyTS.columns)):\n","      dataset = hstack((dataset,t[i]))\n","\n","    # Window Size\n","    n_steps = 5\n","\n","    # convert into input/output\n","    X, y = split_sequences(dataset, n_steps)\n","    n_features = X.shape[2]\n","    \n","\n","\n","\n","\n","    #Vanilla model\n","    def build_LSTM(n_steps, hidden_units):\n","      model = Sequential()\n","      model.add(LSTM(hidden_units, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))\n","      model.add(LSTM(hidden_units, activation='relu'))\n","      model.add(Dense(n_features))\n","      model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","      return model\n","\n","\n","\n","\n","\n","    def FitForecast(X_train, y_train, n_steps, hidden_units, epochs, trained_model):\n","\n","          '''\n","          Fits a model and returns the model and history\n","          Args:\n","              X_train: input dataset for training\n","              y_train: output dataset for training\n","              n_steps: window size\n","              hidden_units: int/list specifying the number \n","                            of hidden units\n","              epochs: int that defines the number of \n","                      training phases through the\n","                      training dataset\n","              trained_model: already trained keras sequential \n","                            model\n","              \n","          Returns:\n","              model: keras sequential model\n","              history: training loss history and training accuracy history\n","              \n","          '''\n","\n","          model = build_LSTM(n_steps,hidden_units)\n","\n","          #initializing with the previous model weights\n","          model.set_weights(weights = trained_model.get_weights())        \n","\n","          history = model.fit(x=X_train, y=y_train, epochs=epochs, verbose=0,shuffle=False)\n","\n","          return model, history\n","\n","\n","\n","    model_withTransfer, hist = FitForecast(X, y, 5, 20, 400, trained_model=model_withTransfer)\n","    print(\"Training completed-------------------->repoID:%d\" %id)\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eg89tSPdr-QQ"},"source":["fig = plt.figure()\n","fig.patch.set_facecolor('white')\n","plt.plot(hist.history['accuracy'])\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train accuracy'], loc='upper right')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7mIL4VPdsUEM"},"source":["fig = plt.figure()\n","fig.patch.set_facecolor('white')\n","plt.plot(hist.history['loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train loss'], loc='upper right')\n","plt.show()"],"execution_count":null,"outputs":[]}]}