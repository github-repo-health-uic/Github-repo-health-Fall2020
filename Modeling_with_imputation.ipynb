{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Modeling.ipynb","provenance":[],"collapsed_sections":["TszHzSufT44_","-rHHSG-czAGG"],"authorship_tag":"ABX9TyMcKzBI97cPNSzYLkEMuuoN"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"i8SkR-HHl7uG","executionInfo":{"status":"ok","timestamp":1605543329933,"user_tz":360,"elapsed":45410,"user":{"displayName":"Dushyant Singh Khinchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhVgT4J-0PZxTmXsjDZlk4iTD5UX35a5leV156K=s64","userId":"14979082350769729724"}},"outputId":"08f8db97-a761-448a-e218-6e2ccf6e484c","colab":{"base_uri":"https://localhost:8080/"}},"source":["# authorize google account to use bigquery\n","from google.colab import auth\n","auth.authenticate_user()\n","print('Authenticated')\n","\n","# authorize drive to pull model\n","from google.colab import drive\n","drive.mount('/content/drive')\n","]\n","# pull data for repo from bigquery\n","project_id = 'uic-capstone-int'\n","from google.cloud import bigquery\n","\n","client = bigquery.Client(project=project_id)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Authenticated\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iO6-xC3VQCHS","executionInfo":{"status":"ok","timestamp":1605543330850,"user_tz":360,"elapsed":1923,"user":{"displayName":"Dushyant Singh Khinchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhVgT4J-0PZxTmXsjDZlk4iTD5UX35a5leV156K=s64","userId":"14979082350769729724"}}},"source":["import os\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import random\n","import numpy as np\n","import datetime\n","from sklearn.metrics import r2_score"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"ofUPrgSD0Mdr","executionInfo":{"status":"ok","timestamp":1605543331872,"user_tz":360,"elapsed":899,"user":{"displayName":"Dushyant Singh Khinchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhVgT4J-0PZxTmXsjDZlk4iTD5UX35a5leV156K=s64","userId":"14979082350769729724"}}},"source":["os.chdir('/content/drive/My Drive/Cloudbakers/Assets/')"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"sSI4yEJSLP7x","executionInfo":{"status":"ok","timestamp":1605547152181,"user_tz":360,"elapsed":326,"user":{"displayName":"Dushyant Singh Khinchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhVgT4J-0PZxTmXsjDZlk4iTD5UX35a5leV156K=s64","userId":"14979082350769729724"}},"outputId":"a0e00af4-1bc9-4c19-c6cb-e59ba5bb3a48","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from datetime import datetime\n","now=datetime.now()\n","today = now.strftime(\"%Y-%m-%d\")\n","today"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'2020-11-16'"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"CI057Hs1mBlN","executionInfo":{"status":"ok","timestamp":1605545509249,"user_tz":360,"elapsed":904,"user":{"displayName":"Dushyant Singh Khinchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhVgT4J-0PZxTmXsjDZlk4iTD5UX35a5leV156K=s64","userId":"14979082350769729724"}}},"source":["WeeklyTS = pd.read_csv('BiggerDS-Part1.csv')\n","WeeklyTS['year'] = WeeklyTS['year'].astype(int)\n","WeeklyTS['month'] = WeeklyTS['month'].astype(int)\n","WeeklyTS['day'] = WeeklyTS['day'].astype(int)\n","WeeklyTS.drop('Unnamed: 0', axis=1, inplace=True)\n","WeeklyTS = WeeklyTS[WeeklyTS.year > 2014]\n","WeeklyTS['dateInt']=WeeklyTS['year'].astype(str) + WeeklyTS['month'].astype(str).str.zfill(2)+ WeeklyTS['day'].astype(str).str.zfill(2)\n","WeeklyTS['Date'] = pd.to_datetime(WeeklyTS['dateInt'], format='%Y%m%d')\n","WeeklyTS.drop(['year','month','day','dateInt'], axis=1, inplace=True)\n","x =pd.DataFrame(WeeklyTS.groupby('repoID').count())\n","x = x[x.Risk_Score>90].index.astype(int).tolist()\n","WeeklyTS = WeeklyTS[WeeklyTS.repoID.isin(x)]"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"zCrmJNzZ1yJZ","executionInfo":{"status":"ok","timestamp":1605545510565,"user_tz":360,"elapsed":289,"user":{"displayName":"Dushyant Singh Khinchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhVgT4J-0PZxTmXsjDZlk4iTD5UX35a5leV156K=s64","userId":"14979082350769729724"}},"outputId":"e46ec0d2-099d-4274-f349-a95f4a862628","colab":{"base_uri":"https://localhost:8080/","height":402}},"source":["WeeklyTS"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>repoID</th>\n","      <th>Risk_Score</th>\n","      <th>Date</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>110</th>\n","      <td>46566064.0</td>\n","      <td>1.112586</td>\n","      <td>2015-11-22</td>\n","    </tr>\n","    <tr>\n","      <th>111</th>\n","      <td>46566064.0</td>\n","      <td>1.420722</td>\n","      <td>2015-11-29</td>\n","    </tr>\n","    <tr>\n","      <th>112</th>\n","      <td>46566064.0</td>\n","      <td>3.177162</td>\n","      <td>2015-12-06</td>\n","    </tr>\n","    <tr>\n","      <th>113</th>\n","      <td>46566064.0</td>\n","      <td>599.740572</td>\n","      <td>2015-12-13</td>\n","    </tr>\n","    <tr>\n","      <th>114</th>\n","      <td>46566064.0</td>\n","      <td>1188.086705</td>\n","      <td>2015-12-20</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>72165</th>\n","      <td>20287787.0</td>\n","      <td>0.000000</td>\n","      <td>2020-08-23</td>\n","    </tr>\n","    <tr>\n","      <th>72166</th>\n","      <td>20287787.0</td>\n","      <td>0.000000</td>\n","      <td>2020-08-30</td>\n","    </tr>\n","    <tr>\n","      <th>72167</th>\n","      <td>20287787.0</td>\n","      <td>0.000000</td>\n","      <td>2020-09-06</td>\n","    </tr>\n","    <tr>\n","      <th>72168</th>\n","      <td>20287787.0</td>\n","      <td>0.000000</td>\n","      <td>2020-09-13</td>\n","    </tr>\n","    <tr>\n","      <th>72169</th>\n","      <td>20287787.0</td>\n","      <td>3.373408</td>\n","      <td>2020-09-20</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>67367 rows Ã— 3 columns</p>\n","</div>"],"text/plain":["           repoID   Risk_Score       Date\n","110    46566064.0     1.112586 2015-11-22\n","111    46566064.0     1.420722 2015-11-29\n","112    46566064.0     3.177162 2015-12-06\n","113    46566064.0   599.740572 2015-12-13\n","114    46566064.0  1188.086705 2015-12-20\n","...           ...          ...        ...\n","72165  20287787.0     0.000000 2020-08-23\n","72166  20287787.0     0.000000 2020-08-30\n","72167  20287787.0     0.000000 2020-09-06\n","72168  20287787.0     0.000000 2020-09-13\n","72169  20287787.0     3.373408 2020-09-20\n","\n","[67367 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"jZxg2zUQmu1L","executionInfo":{"status":"ok","timestamp":1605543371581,"user_tz":360,"elapsed":595,"user":{"displayName":"Dushyant Singh Khinchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhVgT4J-0PZxTmXsjDZlk4iTD5UX35a5leV156K=s64","userId":"14979082350769729724"}}},"source":["#Concatenating smaller and larger dataset for training\n","SmallerDS = pd.read_csv('WeeklyTS.csv')\n","WeeklyTS['Date'] = WeeklyTS['Date'].dt.strftime('%Y-%m-%d')\n","type(WeeklyTS['Date'].iloc[0])\n","WeeklyTS = pd.concat([WeeklyTS,SmallerDS])\n","WeeklyTS.reset_index(level=0, inplace=True)\n","WeeklyTS.drop(['Unnamed: 0','index','level_0'], axis=1, inplace=True)\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"w2IjYGCb8-SJ","executionInfo":{"status":"ok","timestamp":1605543375405,"user_tz":360,"elapsed":241,"user":{"displayName":"Dushyant Singh Khinchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhVgT4J-0PZxTmXsjDZlk4iTD5UX35a5leV156K=s64","userId":"14979082350769729724"}},"outputId":"71affa96-870d-4162-c88b-c056efebcabd","colab":{"base_uri":"https://localhost:8080/","height":402}},"source":["WeeklyTS"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>repoID</th>\n","      <th>Risk_Score</th>\n","      <th>Date</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>110</td>\n","      <td>46566064.0</td>\n","      <td>1.112586</td>\n","      <td>2015-11-22</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>111</td>\n","      <td>46566064.0</td>\n","      <td>1.420722</td>\n","      <td>2015-11-29</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>112</td>\n","      <td>46566064.0</td>\n","      <td>3.177162</td>\n","      <td>2015-12-06</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>113</td>\n","      <td>46566064.0</td>\n","      <td>599.740572</td>\n","      <td>2015-12-13</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>114</td>\n","      <td>46566064.0</td>\n","      <td>1188.086705</td>\n","      <td>2015-12-20</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>67362</th>\n","      <td>72165</td>\n","      <td>20287787.0</td>\n","      <td>0.000000</td>\n","      <td>2020-08-23</td>\n","    </tr>\n","    <tr>\n","      <th>67363</th>\n","      <td>72166</td>\n","      <td>20287787.0</td>\n","      <td>0.000000</td>\n","      <td>2020-08-30</td>\n","    </tr>\n","    <tr>\n","      <th>67364</th>\n","      <td>72167</td>\n","      <td>20287787.0</td>\n","      <td>0.000000</td>\n","      <td>2020-09-06</td>\n","    </tr>\n","    <tr>\n","      <th>67365</th>\n","      <td>72168</td>\n","      <td>20287787.0</td>\n","      <td>0.000000</td>\n","      <td>2020-09-13</td>\n","    </tr>\n","    <tr>\n","      <th>67366</th>\n","      <td>72169</td>\n","      <td>20287787.0</td>\n","      <td>3.373408</td>\n","      <td>2020-09-20</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>67367 rows Ã— 4 columns</p>\n","</div>"],"text/plain":["       index      repoID   Risk_Score        Date\n","0        110  46566064.0     1.112586  2015-11-22\n","1        111  46566064.0     1.420722  2015-11-29\n","2        112  46566064.0     3.177162  2015-12-06\n","3        113  46566064.0   599.740572  2015-12-13\n","4        114  46566064.0  1188.086705  2015-12-20\n","...      ...         ...          ...         ...\n","67362  72165  20287787.0     0.000000  2020-08-23\n","67363  72166  20287787.0     0.000000  2020-08-30\n","67364  72167  20287787.0     0.000000  2020-09-06\n","67365  72168  20287787.0     0.000000  2020-09-13\n","67366  72169  20287787.0     3.373408  2020-09-20\n","\n","[67367 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"ViUUZanEAphy"},"source":["def imputation(WeeklyTS,i):\n","\n","    y = WeeklyTS[WeeklyTS.repoID==i].sort_values('Date')\n","    y['Date']= pd.to_datetime(y['Date'])\n","    y.index=y['Date']\n","    y.drop('Date',axis=1,inplace=True)\n","    idx = pd.date_range('2015-01-01', today, freq='W-SUN')\n","    y.index = pd.DatetimeIndex(y.index)\n","    y = y.reindex(idx, fill_value=0)\n","\n","    y['Risk_Score']=y['Risk_Score'].replace({0:np.nan})\n","    y.drop('repoID', axis=1, inplace=True)\n","    y = y.assign(InterpolateLinear=y.Risk_Score.interpolate(method='linear', limit_direction='both'))\n","    y = y.assign(InterpolateTime=y.Risk_Score.interpolate(method='time', limit_direction='both'))\n","    y = y.assign(InterpolateQuadratic=y.Risk_Score.interpolate(method='quadratic',limit_direction='both'))\n","    y = y.assign(InterpolateCubic=y.Risk_Score.interpolate(method='cubic',limit_direction='both'))\n","    y = y.assign(InterpolateSLinear=y.Risk_Score.interpolate(method='slinear',limit_direction='both'))\n","    y = y.assign(InterpolateAkima=y.Risk_Score.interpolate(method='akima',limit_direction='both'))\n","    y = y.assign(InterpolatePoly5=y.Risk_Score.interpolate(method='polynomial', order=5, limit_direction='both')) \n","    y = y.assign(InterpolatePoly7=y.Risk_Score.interpolate(method='polynomial', order=7,limit_direction='both'))\n","    y = y.assign(InterpolateSpline3=y.Risk_Score.interpolate(method='spline', order=3, limit_direction='both'))\n","    y = y.assign(InterpolateSpline4=y.Risk_Score.interpolate(method='spline', order=4,limit_direction='both'))\n","    y = y.assign(InterpolateSpline5=y.Risk_Score.interpolate(method='spline', order=5,limit_direction='both'))\n","    y = y.assign(FillMean=y.Risk_Score.fillna(y.Risk_Score.mean())) # imputing using the mean\n","    y = y.assign(FillMedian=y.Risk_Score.fillna(y.Risk_Score.median())) # imputing using the median\n","    y = y.assign(RollingMean=y.Risk_Score.fillna(y.Risk_Score.rolling(24,min_periods=1,).mean())) # imputing using the rolling mean\n","    y = y.assign(RollingMedian=y.Risk_Score.fillna(y.Risk_Score.rolling(24,min_periods=1,).median()))# imputing using the rolling median\n","\n","    y.fillna(0, inplace=True)\n","\n","    results = [(method, r2_score(y['Risk_Score'].fillna(0), y[method])) for method in list(y)[3:]]\n","    results_df = pd.DataFrame(np.array(results), columns=['Method', 'R_squared'])\n","    result_df = results_df.sort_values(by='R_squared', ascending=False)\n","    column_name =result_df.iloc[0].Method\n","    y=y[[column_name]]\n","    y.reset_index(level=0, inplace=True)\n","    y['Date'] = y['index'].dt.strftime('%Y-%m-%d')\n","    y.drop('index',axis=1, inplace=True)\n","    y.rename(columns={column_name:'Risk_Score'},inplace=True)\n","    y['repoID']=i\n","    return y\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bFG6iSES6AkU"},"source":["repo_ids = WeeklyTS['repoID'].unique().tolist()\n","random.shuffle(repo_ids)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6QEOwvOZAWZa"},"source":["WTS=pd.DataFrame()\n","\n","for i in range(0,len(repo_ids)):\n","   \n","      impute = imputation(WeeklyTS,repo_ids[i])\n","      WTS=WTS.append(impute)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z8Q2bEXPTN39"},"source":["WeeklyTS = WTS"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cfYW-06C5hV4"},"source":["ScaledDS = pd.DataFrame()\n","\n","for i in range(0,len(repo_ids)):\n","   TransformDS = WeeklyTS[WeeklyTS.repoID==repo_ids[i]]\n","   max = TransformDS['Risk_Score'].max()\n","   min = TransformDS['Risk_Score'].min()\n","   div = max - min\n","   scaled_risk_score = ((TransformDS.Risk_Score-min)/div)*10\n","   scaled_risk_score = pd.DataFrame(scaled_risk_score)\n","   scaled_risk_score.rename(columns={'Risk_Score':'Scaled_Risk_Score'}, inplace=True)\n","   scaledDS = pd.concat([TransformDS,scaled_risk_score],axis=1)\n","   ScaledDS = ScaledDS.append(scaledDS)\n","WeeklyTS = ScaledDS\n","WeeklyTS.drop('Risk_Score', axis=1, inplace=True)\n","WeeklyTS.rename(columns={'Scaled_Risk_Score':'Risk_Score'}, inplace=True)\n","WeeklyTS.reset_index(level=0,inplace=True)\n","WeeklyTS.drop('index',axis=1, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PZxWUoKTXwLT"},"source":["# # Removing repositories with a lot of zeroes\n","# a = WeeklyTS.repoID.unique().tolist()\n","# m=WeeklyTS[WeeklyTS.Risk_Score!=0].groupby('repoID').count()\n","# repo_to_remove=m[m.Risk_Score<30].index.tolist()\n","# l3 = [x for x in a if x not in repo_to_remove]\n","# WeeklyTS = WeeklyTS[WeeklyTS.repoID.isin(l3)]\n","# WeeklyTS.reset_index(level=0, inplace=True)\n","# WeeklyTS"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z1AK8HG30Cj5"},"source":["# Modeling 2"]},{"cell_type":"code","metadata":{"id":"vPO2UWAv0GhL"},"source":["import os\n","import pandas as pd\n","import numpy as np\n","from datetime import datetime\n","from tensorflow import keras\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JJaobm65SayW"},"source":["## preprocession\n"]},{"cell_type":"code","metadata":{"id":"1hV_7NyVSef-"},"source":["import numpy as np\n","import pandas as pd\n","\n","from sklearn.preprocessing import MinMaxScaler\n","\n","\n","def preprocessing(series):\n","    '''\n","    MinMax Scaling of the raw time series\n","    Args:\n","        series: the raw time series\n","    Returns:\n","        scaled_series and scaler object\n","    '''\n","    series = np.array(series)\n","    scaler = MinMaxScaler(feature_range=(0, 1))\n","    scaled = scaler.fit_transform(series.reshape(-1,1))\n","    scaled_series = scaled.reshape((len(series),))\n","    \n","    return scaled_series, scaler\n","\n","def inverse_transform(series,scaler):\n","    '''\n","    Inverse transform of scales series\n","    Args:\n","        series: scaled series\n","        scaler: scaler object\n","    Returns:\n","        unscaled series\n","    '''\n","    return scaler.inverse_transform(series.reshape(-1,1))\n","\n","def getSeries(data,p):\n","    '''\n","    Splits a given time series proportionally\n","    for training and testing purposes\n","    \n","    Args:\n","        data: numpy array or pandas series \n","              containing the time series.\n","        p: float value that defines the \n","           proportion of the series used\n","           for training.\n","    Returns:\n","        series: time series for training\n","        y_test: time series for testing\n","        n_test: number of timesteps \n","                in the test series\n","    \n","    '''\n","    n = data.shape[0]\n","    n_train = int(n * p) \n","    n_test = n - n_train\n","\n","    x = np.arange(n)\n","        \n","    index_train = x[:n_train]\n","    index_test = x[n_train:]\n","    \n","    series = data[index_train]\n","\n","    y_test = data[index_test]\n","    return series, y_test, n_test\n","\n","def getInputOutput(series, input_size):\n","    '''\n","    Transforms the time series into desired \n","    shape to be able to pass to the network\n","    \n","    Args:\n","        series: the time series.\n","        input_size: int that defines the length \n","                    of the input sequence to be \n","                    fed to the network\n","    Returns:\n","        X_train: input dataset\n","        y_train: output values\n","        X_test: the last available sequence\n","    \n","    '''\n","    \n","    series = np.array(series)\n","    xlen = len(series)\n","    xrows = xlen - input_size\n","    \n","    X_train, y_train = [], []\n","    \n","    for i in range(xrows):\n","        j = i + input_size\n","        a = series[i:j, np.newaxis]\n","        X_train.append(a)\n","        y_train.append(series[j])\n","    \n","    X_train,y_train = np.array(X_train), np.array(y_train)\n","    X_test = series[xrows:].reshape(1,input_size,1)\n","    \n","    \n","    return X_train, y_train, X_test"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oKnlccfySno8"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"slg4UU9jCq2y"},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import keras\n","import itertools\n","\n","from sklearn.metrics import mean_squared_error\n","from sklearn.preprocessing import MinMaxScaler\n","from keras.models import Sequential\n","from keras.layers import Dense, LSTM, Dropout\n","\n","\n","\n","\n","def build_LSTM(input_size,hidden_units, dropout, learning_rate):\n","    '''\n","    Builds the Network with LSTM hidden layers\n","    \n","    Args:\n","        input_size: int that defines the length \n","                    of the input sequence to be \n","                    fed to the network\n","        hidden_units: int/list specifying the number \n","                      of hidden units in the hidden \n","                      layer/layers\n","        dropout: boolean specifing whether to add dropout\n","                 with 0.5 rate per layer\n","        learning_rate: learning rate of the Adam \n","                       optimization algorithm\n","    Returns:\n","        model: keras sequential model\n","        \n","    '''\n","    h = hidden_units\n","    \n","\n","    model = Sequential()\n","    \n","    if isinstance(h,list):\n","    \n","        model.add(LSTM(h[0],\n","                   batch_input_shape=(1,input_size, 1), \n","                   return_sequences=True, \n","                   stateful=True))\n","                  \n","        if dropout:\n","            model.add(Dropout(rate=0.5))\n","\n","        if len(h) > 2:\n","            #removing 1st and last units\n","            for index, units in enumerate(h[1:-1]):  \n","                model.add(LSTM(units, \n","                               batch_input_shape=(1,h[index], 1), \n","                               return_sequences=True, \n","                               stateful=True)) \n","                if dropout:\n","                    model.add(Dropout(rate=0.5))\n","\n","        model.add(LSTM(h[-1], \n","                       batch_input_shape=(1,h[-2], 1), \n","                       return_sequences=False, \n","                       stateful=True))\n","        if dropout:\n","            model.add(Dropout(rate=0.5))\n","    else:\n","        model.add(LSTM(h, \n","                   batch_input_shape=(1,input_size, 1), \n","                   return_sequences=False, \n","                   stateful=True)) \n","        if dropout:\n","            model.add(Dropout(rate=0.5))\n","        \n","    \n","    model.add(Dense(1))\n","    adam = keras.optimizers.Adam(lr=learning_rate)\n","    model.compile(loss='mse', optimizer=adam)\n","    return model\n","\n","def predict_ahead(model,X_test,n_ahead):\n","    '''\n","    Makes predictions based on the last available sequence\n","    \n","    Args:\n","        model: keras sequential model\n","        X_test: the last available sequence\n","        n_ahead: number of predictions to make \n","        \n","    Returns:\n","        predictions: numpy array containing the predicted values\n","        \n","    '''    \n","    predictions = np.zeros(n_ahead)\n","    predictions[0] = model.predict(X_test,batch_size = 1)\n","    \n","    if n_ahead > 1:\n","        for i in range(1,n_ahead):\n","            x_new = np.append(X_test[0][1:],predictions[i-1])\n","            X_test = x_new.reshape(1,x_new.shape[0],1)\n","            predictions[i] = model.predict(X_test,batch_size = 1)\n","    return predictions\n","\n","def FitForecast(X_train, y_train, X_test, n_ahead, input_size,\n","                hidden_units, dropout, val_split, learning_rate, \n","                epochs, trained_model):\n","    \n","    '''\n","    Fits a model and returns the predicted values. \n","    Optionally weights from another network can be passed \n","    \n","    Args:\n","        X_train: input dataset for training\n","        y_train: output dataset for training\n","        X_test: the last available sequence\n","        n_ahead: number of predictions to make \n","        time_series: the time series of interest\n","        input_size: int that defines the length \n","                    of the input sequence to be \n","                    fed to the network\n","        hidden_units: int/list specifying the number \n","                      of hidden units in the hidden \n","                      layer/layers\n","        dropout: boolean specifing whether to add dropout\n","                 with 0.5 rate per layer\n","        learning_rate: learning rate of the Adam \n","                       optimization algorithm\n","        epochs: int that defines the number of \n","                training phases through the\n","                training dataset\n","        trained_model: already trained keras sequential \n","                       model\n","        \n","    Returns:\n","        model: keras sequential model\n","        predictions: numpy array containing the predicted values\n","        history: training and validation loss history\n","        \n","    '''\n","    model = build_LSTM(input_size,hidden_units,dropout, learning_rate)\n","    \n","    if trained_model is not None:\n","        model.set_weights(weights = trained_model.get_weights())        \n","    \n","    \n","    history = model.fit(x=X_train, y=y_train, \n","                batch_size=1, epochs=epochs, \n","                verbose=1, validation_split=val_split,\n","                shuffle=False)\n","\n","    predictions = predict_ahead(model,X_test,n_ahead)\n","    return model, predictions, history\n","\n","def FitEvaluate(time_series,params):\n","    '''\n","    Calls the pipeline to fit an LSTM model to the \n","    given time series\n","    \n","    Args:\n","        time_series: the time series of interest\n","        params: a dictionary specifying parameters\n","                {input_size, hidden_units, dropout,\n","                learning_rate, n_ahead, val_split, \n","                epochs, verbose, plot}\n","    Returns:\n","        model: keras sequential model      \n","        mse: mean squared error of the prediction\n","        history: training and validation loss history\n","        \n","    '''   \n","    \n","    for k in params.keys():\n","        globals()[k] = params[k]\n","    \n","    \n","    scaled_series, scaler = preprocessing(time_series)\n","    series, y_test, n_test = getSeries(scaled_series,0.8)\n","    X_train,y_train,X_test = getInputOutput(series,input_size)\n","    \n","    # show only n_ahead number of actual values\n","    y_test = y_test[np.arange(n_ahead)]\n","\n","    new_model, predictions, history = FitForecast(X_train,y_train,X_test,n_ahead,\n","                                        input_size,hidden_units,dropout, val_split,\n","                                        learning_rate,epochs,trained_model=None)\n","    \n","    # rescaling\n","    series = inverse_transform(series, scaler)\n","    y_test = inverse_transform(y_test, scaler)\n","    predictions = inverse_transform(predictions, scaler)\n","    \n","    mse = mean_squared_error(y_true=y_test,y_pred=predictions)\n","    \n","    if verbose:\n","        print('\\n')\n","        print('======== Prediction Evaluation =========')\n","        print('MSE is {}'.format(round(mse,4)))\n","        \n","    if plot:\n","        ViewLoss(history)\n","        view_predictions(series,predictions,y_test,'Actual vs Forecast')\n","    return new_model, mse, history, predictions, y_test\n","\n","def TransferLearning(time_series,params,model):\n","    '''\n","    Calls the pipeline to fit an LSTM model to the \n","    given time series with and without knowledge\n","    transfer\n","    \n","    Args:\n","        time_series: the time series of interest\n","        params: a dictionary specifying parameters\n","                {input_size, hidden_units, dropout,\n","                learning_rate, n_ahead, val_split, \n","                epochs, verbose, plot}\n","        model: already trained keras sequential \n","               model\n","    Returns:\n","        mean squared errors of the predictions\n","        and 2 plots         \n","    '''\n","    for k in params.keys():\n","        globals()[k] = params[k]\n","    \n","    val_split = 0\n","    \n","    scaled_series, scaler = preprocessing(time_series)\n","    series, y_test, n_test = getSeries(scaled_series,0.8)\n","    X_train,y_train,X_test = getInputOutput(series,input_size)\n","\n","    # show only n_ahead number of actual values\n","    y_test = y_test[np.arange(n_ahead)]\n","\n","    \n","    print('*** Fitting a model without knowledge transfer ***')\n","    model_noTransfer, predictions_noTransfer, _ = FitForecast(X_train,y_train,\n","                                                             X_test,n_ahead,\n","                                               input_size,hidden_units,\n","                                                             dropout,val_split,\n","                                                             learning_rate,\n","                                               epochs,\n","                                                             trained_model=None)\n","    \n","    print('\\n')\n","    print('*** Fitting a model with knowledge transfer ***')\n","    model_withTransfer, predictions_withTransfer, _ = FitForecast(X_train,y_train,\n","                                                                 X_test,n_ahead,\n","                                                                 input_size,hidden_units,\n","                                                                 dropout,val_split,\n","                                                                 learning_rate,\n","                                                                 epochs,\n","                                                                 trained_model=model)\n","    \n","    # rescaling\n","    series = inverse_transform(series, scaler)\n","    y_test = inverse_transform(y_test, scaler)\n","    predictions_noTransfer = inverse_transform(predictions_noTransfer, scaler)\n","    predictions_withTransfer = inverse_transform(predictions_withTransfer, scaler)\n","    \n","    mse_noTransfer = mean_squared_error(y_true=y_test,y_pred=predictions_noTransfer)\n","    mse_withTransfer = mean_squared_error(y_true=y_test,y_pred=predictions_withTransfer)\n","      \n","    print('\\n')\n","    print('======== Results for no knowledge transfer =========')\n","    print('The RMSE is {}'.format(round(np.sqrt(mse_noTransfer),4)))\n","    print('\\n')\n","    print('======== Results for knowledge transfer =========')\n","    print('The RMSE is {}'.format(round(np.sqrt(mse_withTransfer),4)))\n","        \n","    view_predictions(series,predictions_noTransfer,y_test,title='Without Transfer')\n","    view_predictions(series,predictions_withTransfer,y_test,title='With Transfer')\n","\n","    return model_withTransfer\n","\n","def GridSearch(series,params_grid):\n","    '''\n","    Runs a grid search over specified parameter ranges\n","    Args: \n","        series: the time series of interest\n","        params_grid: a dictionary specifying parameters\n","                    {input_size, hidden_units, dropout,\n","                    learning_rate, n_ahead, val_split, \n","                    epochs, verbose, plot} and their \n","                    possible value ranges\n","    Returns:\n","        model: the model with the lowest MSE \n","        logs: logs of all combinations        \n","       \n","    '''\n","    param_names = list(params_grid.keys())\n","    param_values = list(params_grid.values()) \n","    combinations = list(itertools.product(*param_values))\n","    \n","    logs = pd.DataFrame(combinations,columns=param_names)\n","    \n","    mse_prev = 50\n","    for index, comb in enumerate(combinations):\n","        \n","        print('Fitting {}/{} model'.format(index+1,len(combinations)))\n","        params = dict(zip(param_names,comb))\n","        model, mse, history, predn, y_test = FitEvaluate(series,params)\n","        train_loss = history.history['loss']\n","        val_loss = history.history['val_loss']\n","        \n","        \n","        if mse < mse_prev:\n","            mse_prev = mse\n","            best_model = model\n","            best_predictions = predn\n","            yt = y_test\n","        \n","        logs.at[index,'mse'] = mse\n","        logs.at[index,'mean_training_loss'] = np.mean(train_loss)\n","        logs.at[index,'std_training_loss'] = np.std(train_loss)\n","        logs.at[index,'mean_val_loss'] = np.mean(val_loss)\n","        logs.at[index,'std_val_loss'] = np.std(val_loss)\n","    logs.to_csv('results.csv',index=False)\n","    # view_predictions(series,best_predictions,y_test,'Actual vs Forecast')\n","    return best_model, logs, best_predictions, yt\n","\n","\n","def generalTuning(series1, series2, params):\n","    \n","    '''\n","    Fitting 3 models trained on \n","    {general domain,general+in-domain,in-domain only}\n","    and comparing them\n","    Args: \n","        series1: target related time series \n","        series2: target time series\n","        params: a dictionary specifying parameters\n","                {input_size, hidden_units, dropout,\n","                learning_rate, n_ahead, val_split, \n","                epochs, verbose, plot}\n","    Returns:\n","        mean squared errors of the predictions\n","        and 3 plots \n","    \n","    '''\n","    \n","    time_series = np.concatenate([series1,series2])\n","        \n","    \n","    \n","    for k in params.keys():\n","        globals()[k] = params[k]\n","        \n","    val_split = 0\n","    \n","    # preprocessing general series\n","    scaled_general, scaler_general = preprocessing(time_series)\n","    series_general, _, __ = getSeries(scaled_general,0.9)\n","    X_train_general,y_train_general,___ = getInputOutput(series_general,input_size)\n","\n","    # preprocessing the target series\n","    scaled_target, scaler_target = preprocessing(series2) \n","    series_target, y_test_target, n_test = getSeries(scaled_target,0.8)\n","    X_train_target,y_train_target,X_test_target = getInputOutput(series_target,input_size)\n","\n","    # comparing predictions with only n_ahead number of actual values\n","    y_test_target = y_test_target[np.arange(n_ahead)]\n","\n","    \n","    # build and train a model on the general domain (time_series)\n","    \n","    print('*** Fitting a model on general domain ***')\n","\n","    \n","    model_general, predictions_pre_tuned, hist = FitForecast(X_train_general,y_train_general,\n","                                                             X_test_target,n_ahead,\n","                                                             input_size, hidden_units,\n","                                                             dropout, val_split,\n","                                                             learning_rate,\n","                                                             epochs,\n","                                                             trained_model=None)                                               \n","                                                             \n","                                                             \n","                                                        \n"," \n","    # initiallize a model for target\n","    model_tuned = build_LSTM(input_size, hidden_units, dropout, learning_rate)\n","        \n","    # transfer the knowledge from the pre-trained model\n","    # and tune it only on the target domain (series2)\n","    model_tuned.set_weights(weights=model_general.get_weights())        \n","    \n","    \n","    print('\\n *** Tuning a model on target domain ***')\n","\n","    model_tuned.fit(x=X_train_target, y=y_train_target, \n","                        batch_size=1, epochs=epochs, \n","                        verbose=1, validation_data=None,\n","                        shuffle=False)\n","    \n","    predictions_tuned = predict_ahead(model_tuned,X_test_target,n_ahead)\n","    \n","    \n","   \n","    print('\\n *** Fitting a model on target domain only ***')\n","\n","    model_target, predictions_target,hist2 = FitForecast(X_train_target,y_train_target,\n","                                                   X_test_target,n_ahead,\n","                                                   input_size, hidden_units,\n","                                                   dropout, val_split,\n","                                                   learning_rate,\n","                                                   2 * epochs,\n","                                                   trained_model=None)                                               \n","\n","\n","    series_target = inverse_transform(series_target,scaler_target)\n","    y_test_target = inverse_transform(y_test_target,scaler_target)\n","    \n","    series_general = inverse_transform(series_general,scaler_general)\n","    predictions_pre_tuned = inverse_transform(predictions_pre_tuned,scaler_target)\n","    mse_pre_tuned = mean_squared_error(y_true=y_test_target,y_pred=predictions_pre_tuned)\n","    \n","    predictions_tuned = inverse_transform(predictions_tuned,scaler_target)\n","    mse_tuned = mean_squared_error(y_true=y_test_target,y_pred=predictions_tuned)\n","    \n","    predictions_target = inverse_transform(predictions_target,scaler_target)  \n","    mse_target = mean_squared_error(y_true=y_test_target,y_pred=predictions_target)\n","    \n","    print('\\n')\n","    print('======== Results for pre_tuned model =========')\n","    print('The RMSE is {}'.format(round(np.sqrt(mse_pre_tuned),4)))\n","    print('\\n')\n","    print('======== Results for tuned model =========')\n","    print('The RMSE is {}'.format(round(np.sqrt(mse_tuned),4)))\n","    \n","    print('\\n')\n","    print('======== Results for target model only =========')\n","    print('The RMSE is {}'.format(round(np.sqrt(mse_target),4)))\n","    \n","        \n","    view_predictions([series_general,series_target],predictions_pre_tuned,y_test_target,title='Pre-tuned')\n","    view_predictions([series_general,series_target],predictions_tuned,y_test_target,title='Tuned')\n","    view_predictions(series_target,predictions_target,y_test_target,title='Target only')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TszHzSufT44_"},"source":["## Visualization"]},{"cell_type":"code","metadata":{"id":"J35G9GVeT1rg"},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","from statsmodels.tsa.stattools import acf, pacf\n","from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n","\n","def ACF(series,lags):\n","    '''\n","    Plots the Autocorrelation graph\n","    of the series\n","    Args:\n","        series: time series of interest\n","        lags: int specifying number of \n","              periods to look back\n","    Returns: plot\n","    '''\n","    plot_acf(series, lags=lags)\n","    plt.savefig('ACF.png')\n","    plt.show()\n","\n","def plotSeries(series_dict):\n","    '''\n","    Plots several time series\n","    Args:\n","        series_dict: dict with series names as keys\n","                     and time series as values\n","    Returns:\n","        plot\n","        \n","    '''\n","    n_series = len(series_dict) - 1\n","    names = list(series_dict.keys())\n","    \n","    plt.figure(figsize=(10,20))\n","    \n","    for i in range(n_series):\n","        plt.subplot(n_series,1,i+1)\n","        plt.title(names[i+1])\n","        plt.plot(series_dict['t'],series_dict[names[i+1]])\n","        \n","    plt.show()\n","\n","def ViewLoss(history):\n","    '''\n","    Plots the history of model training\n","    '''\n","    plt.plot(history.history['loss'],label='Train')\n","    plt.plot(history.history['val_loss'],label='Val')\n","    plt.title('Model loss')\n","    plt.ylabel('Loss')\n","    plt.xlabel('Epoch')\n","    plt.legend()\n","    plt.savefig('loss-history.png')\n","    plt.show()\n","\n","def view_predictions(series,predictions,actual,title):\n","    '''\n","    Plots the results of the predictions made by the model\n","    \n","    Args:\n","        series: the time series used for training the network\n","        predictions: numpy array containing the predicted values\n","        actual: the actual time series not seen by the network\n","        title: the title of the plot\n","        \n","    Returns:\n","        plot        \n","        \n","    '''  \n","    \n","    plt.figure(figsize=(8,4))\n","    plt.title(title)\n","    \n","    if isinstance(series,list):\n","        train_index = np.arange(len(series[0]))\n","        test_index = len(series[0]) + np.arange(len(actual))\n","        \n","        plt.plot(train_index,series[0], label = 'general')\n","        \n","    else:\n","        train_index = np.arange(len(series))\n","        test_index = len(series) + np.arange(len(actual))        \n","        plt.plot(train_index,series,label = 'training')\n","\n","    if len(predictions) > 4:\n","        plt.plot(test_index,predictions,label = 'prediction',color='g')\n","        plt.plot(test_index,actual,label = 'actual',color='orange')\n","    else:\n","        plt.scatter(test_index,predictions,label = 'prediction',color='g')\n","        plt.scatter(test_index,actual,label = 'actual',color='orange')    \n","    \n","    plt.xlabel('Index')\n","    plt.ylabel('Data')\n","    \n","    plt.legend(loc='upper left')\n","    plt.savefig('{}_{}.png'.format(title,len(series)))\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fITWMpGemcDq"},"source":["## Grid Search"]},{"cell_type":"code","metadata":{"id":"PF9ici9h0_yZ"},"source":["WeeklyTS = pd.read_csv('/content/drive/My Drive/Cloudbakers/Assets/WeeklyTS.csv')\n","WeeklyTS.drop('Unnamed: 0', axis=1, inplace=True)\n","WeeklyTS = WeeklyTS[WeeklyTS.year > 2014]\n","WeeklyTS['dateInt']=WeeklyTS['year'].astype(str) + WeeklyTS['month'].astype(str).str.zfill(2)+ WeeklyTS['day'].astype(str).str.zfill(2)\n","WeeklyTS['Date'] = pd.to_datetime(WeeklyTS['dateInt'], format='%Y%m%d')\n","WeeklyTS.drop(['year','month','day','dateInt'], axis=1, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1GbugO7g23K7","executionInfo":{"status":"ok","timestamp":1604630969629,"user_tz":360,"elapsed":247,"user":{"displayName":"Dushyant Singh Khinchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhVgT4J-0PZxTmXsjDZlk4iTD5UX35a5leV156K=s64","userId":"14979082350769729724"}},"outputId":"ca422017-72c4-45be-8774-a9d4460a3f33","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["now=datetime.now()\n","today = now.strftime(\"%Y-%m-%d\")\n","today"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'2020-11-06'"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"qC5BxCaJ3MBn"},"source":["repo_ids = WeeklyTS['repoID'].unique().tolist()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NUIKONii3bmz"},"source":["Sub_WeeklyTS = WeeklyTS[WeeklyTS['repoID']==23122983].sort_values(by=['Date'])\n","Sub_WeeklyTS.index=Sub_WeeklyTS['Date']\n","Sub_WeeklyTS.drop('Date',axis=1,inplace=True)\n","idx = pd.date_range('2015-01-01', today, freq='W-SUN')\n","Sub_WeeklyTS.index = pd.DatetimeIndex(Sub_WeeklyTS.index)\n","Sub_WeeklyTS = Sub_WeeklyTS.reindex(idx, fill_value=0)\n","Sub_WeeklyTS.drop(['repoID'], axis=1, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kg_kt-kb39is"},"source":["# date = Sub_WeeklyTS.index.strftime(\"%Y-%m-%d\").to_numpy()\n","# # type(date)\n","Risk_Score = Sub_WeeklyTS.Risk_Score.to_numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UGFFCz-l55Js"},"source":["# Sub_Weekly_dict = {'t':date,\n","#               'Risk_Score':Risk_Score}\n","# plotSeries(Sub_Weekly_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dMGAo_2otvqe","executionInfo":{"status":"ok","timestamp":1605062733169,"user_tz":360,"elapsed":321,"user":{"displayName":"Dushyant Singh Khinchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhVgT4J-0PZxTmXsjDZlk4iTD5UX35a5leV156K=s64","userId":"14979082350769729724"}},"outputId":"cf2ec0ca-bdd4-49d1-f54a-046e0c40ad33","colab":{"base_uri":"https://localhost:8080/"}},"source":["Risk_Score"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n","        1.30712982,  4.7714807 ,  3.92138947,  4.8710617 ,  3.74266067,\n","        2.15722105,  9.6425424 ,  1.30712982,  0.        ,  6.53564912,\n","        0.        ,  4.3144421 ,  2.61425965,  4.7714807 ,  2.61425965,\n","        1.30712982,  2.15722105,  3.46435088,  2.43553085,  6.07861053,\n","        0.85009123,  2.61425965,  0.        ,  2.15722105,  1.30712982,\n","        2.61425965,  0.        ,  3.92138947,  0.85009123,  2.61425965,\n","        0.        ,  1.30712982,  2.15722105,  3.92138947,  0.85009123,\n","        2.15722105,  3.46435088,  3.46435088,  3.46435088,  1.30712982,\n","        3.92138947,  2.15722105,  0.        ,  3.92138947,  1.47831409,\n","        2.61425965,  2.61425965,  1.30712982,  2.61425965,  3.28562208,\n","        5.0497905 ,  1.30712982,  0.        ,  2.15722105,  4.7714807 ,\n","        7.20701155,  4.7714807 ,  4.3144421 ,  6.07861053,  3.46435088,\n","        3.46435088,  2.61425965,  1.30712982,  2.61425965,  1.30712982,\n","        3.00731228,  1.70018245,  2.43553085,  3.92138947,  1.30712982,\n","        0.85009123,  7.48532135,  2.43553085,  0.        ,  1.30712982,\n","        4.48562637,  1.30712982,  4.7714807 ,  1.30712982,  1.97849225,\n","        1.30712982,  6.29293436,  2.61425965,  0.        ,  2.15722105,\n","        5.2285193 ,  5.44284313,  0.        ,  2.61425965,  4.7714807 ,\n","        2.61425965,  4.5136041 ,  1.30712982,  0.        ,  1.30712982,\n","        0.34991306,  3.46435088,  4.7714807 ,  2.61425965,  0.        ,\n","        3.85740351,  2.15722105,  0.85009123,  0.        ,  1.30712982,\n","        1.30712982,  2.61425965,  3.46435088, 10.        ,  0.        ,\n","        4.3144421 ,  4.7714807 ,  0.        ,  1.30712982,  3.46435088,\n","        1.30712982,  3.74266067,  1.12840102,  2.15722105,  1.30712982,\n","        0.85009123,  1.70018245,  0.85009123,  0.        ,  1.65704289,\n","        0.        ,  0.        ,  0.85009123,  0.        ,  1.30712982,\n","        1.30712982,  0.85009123,  2.43553085,  0.        ,  1.30712982,\n","        0.        ,  1.30712982,  0.        ,  0.        ,  0.        ,\n","        0.85009123,  3.92138947,  1.30712982,  0.        ,  0.        ,\n","        8.62888421,  1.30712982,  0.        ,  0.        ,  7.38574035,\n","        3.46435088,  1.30712982,  0.        ,  0.        ,  0.        ,\n","        0.        ,  1.30712982,  0.        ,  0.85009123,  0.        ,\n","        1.30712982,  1.30712982,  0.        ,  1.30712982,  0.        ,\n","        0.        ,  0.        ,  0.        ,  1.30712982,  1.30712982,\n","        1.30712982,  0.        ,  3.05055386,  1.30712982,  5.62157193,\n","        0.85009123,  0.        ,  0.        ,  1.30712982,  1.30712982,\n","        2.61425965,  0.85009123,  0.        ,  1.30712982,  0.        ,\n","        0.85009123,  2.15722105,  2.15722105,  2.15722105,  0.        ,\n","        3.92138947,  0.85009123,  2.61425965,  3.00731228,  0.        ,\n","        1.30712982,  0.        ,  0.85009123,  2.61425965,  1.30712982,\n","        0.        ,  1.30712982,  0.        ,  0.        ,  0.        ,\n","        0.        ,  0.        ,  0.        ,  0.        ,  0.85009123,\n","        2.15722105,  1.70018245,  2.15722105,  4.5927519 ,  1.12840102,\n","        1.30712982,  1.30712982,  0.        ,  0.85009123,  0.85009123,\n","        1.30712982,  3.28562208,  2.61425965,  0.85009123,  2.43553085,\n","        1.30712982,  0.        ,  1.30712982,  3.46435088,  0.        ,\n","        0.        ,  3.74266067,  0.        ,  0.        ,  0.        ,\n","        0.        ,  1.30712982,  1.30712982,  1.30712982,  0.        ,\n","        1.30712982,  1.70018245,  2.15722105,  1.30712982,  0.        ,\n","        0.        ,  0.85009123,  0.        ,  0.85009123,  0.        ,\n","        1.97849225,  1.30712982,  0.85009123,  1.12840102,  2.15722105,\n","        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n","        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n","        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n","        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n","        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n","        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n","        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n","        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n","        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n","        0.        ])"]},"metadata":{"tags":[]},"execution_count":209}]},{"cell_type":"code","metadata":{"id":"Uz0qwZCi7QIa","executionInfo":{"status":"error","timestamp":1605061963372,"user_tz":360,"elapsed":1454659,"user":{"displayName":"Dushyant Singh Khinchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhVgT4J-0PZxTmXsjDZlk4iTD5UX35a5leV156K=s64","userId":"14979082350769729724"}},"outputId":"7db00559-58fb-4d59-c3f4-d22ac4f40eb2","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# to perform a grid search over the parameter\n","params_grid = {'input_size': [3,10,15,30,70],\n","              'hidden_units':[100,[100,50],[100,50,50]],\n","              'dropout': [True, False],\n","              'learning_rate':[4e-5],\n","              'n_ahead':[5],\n","              'val_split': [0.2],\n","              'epochs':[20],\n","              'verbose':[False],\n","              'plot':[False]}\n","\n","# Risk_Score = Sub_WeeklyTS['Risk_Score'].to_numpy()\n","model_, logs = GridSearch(Risk_Score,params_grid)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Fitting 1/30 model\n","Epoch 1/20\n","192/192 [==============================] - 1s 4ms/step - loss: 0.0632 - val_loss: 0.0175\n","Epoch 2/20\n","192/192 [==============================] - 0s 2ms/step - loss: 0.0450 - val_loss: 0.0155\n","Epoch 3/20\n","192/192 [==============================] - 0s 2ms/step - loss: 0.0407 - val_loss: 0.0160\n","Epoch 4/20\n","192/192 [==============================] - 0s 2ms/step - loss: 0.0408 - val_loss: 0.0162\n","Epoch 5/20\n","192/192 [==============================] - 0s 2ms/step - loss: 0.0416 - val_loss: 0.0161\n","Epoch 6/20\n","192/192 [==============================] - 0s 2ms/step - loss: 0.0384 - val_loss: 0.0163\n","Epoch 7/20\n","192/192 [==============================] - 0s 2ms/step - loss: 0.0397 - val_loss: 0.0163\n","Epoch 8/20\n","192/192 [==============================] - 0s 2ms/step - loss: 0.0388 - val_loss: 0.0163\n","Epoch 9/20\n","192/192 [==============================] - 0s 2ms/step - loss: 0.0376 - val_loss: 0.0165\n","Epoch 10/20\n","192/192 [==============================] - 0s 3ms/step - loss: 0.0394 - val_loss: 0.0163\n","Epoch 11/20\n","192/192 [==============================] - 0s 2ms/step - loss: 0.0407 - val_loss: 0.0164\n","Epoch 12/20\n","192/192 [==============================] - 0s 2ms/step - loss: 0.0389 - val_loss: 0.0166\n","Epoch 13/20\n","192/192 [==============================] - 0s 2ms/step - loss: 0.0397 - val_loss: 0.0164\n","Epoch 14/20\n","192/192 [==============================] - 0s 2ms/step - loss: 0.0374 - val_loss: 0.0163\n","Epoch 15/20\n","192/192 [==============================] - 0s 2ms/step - loss: 0.0391 - val_loss: 0.0165\n","Epoch 16/20\n","192/192 [==============================] - 0s 2ms/step - loss: 0.0397 - val_loss: 0.0165\n","Epoch 17/20\n","192/192 [==============================] - 0s 2ms/step - loss: 0.0395 - val_loss: 0.0162\n","Epoch 18/20\n","192/192 [==============================] - 0s 3ms/step - loss: 0.0382 - val_loss: 0.0167\n","Epoch 19/20\n","192/192 [==============================] - 0s 2ms/step - loss: 0.0403 - val_loss: 0.0163\n","Epoch 20/20\n","192/192 [==============================] - 0s 2ms/step - loss: 0.0380 - val_loss: 0.0168\n","Fitting 2/30 model\n","Epoch 1/20\n","192/192 [==============================] - 1s 4ms/step - loss: 0.0659 - val_loss: 0.0159\n","Epoch 2/20\n","192/192 [==============================] - 0s 2ms/step - loss: 0.0411 - val_loss: 0.0158\n","Epoch 3/20\n","192/192 [==============================] - 0s 2ms/step - loss: 0.0378 - val_loss: 0.0160\n","Epoch 4/20\n","192/192 [==============================] - 0s 2ms/step - loss: 0.0377 - val_loss: 0.0161\n","Epoch 5/20\n","192/192 [==============================] - 0s 2ms/step - loss: 0.0377 - val_loss: 0.0161\n","Epoch 6/20\n","192/192 [==============================] - 0s 2ms/step - loss: 0.0377 - val_loss: 0.0161\n","Epoch 7/20\n","192/192 [==============================] - 0s 2ms/step - loss: 0.0377 - val_loss: 0.0161\n","Epoch 8/20\n","192/192 [==============================] - 0s 2ms/step - loss: 0.0377 - val_loss: 0.0161\n","Epoch 9/20\n","192/192 [==============================] - 0s 2ms/step - loss: 0.0377 - val_loss: 0.0161\n","Epoch 10/20\n","192/192 [==============================] - 0s 2ms/step - loss: 0.0377 - val_loss: 0.0162\n","Epoch 11/20\n","192/192 [==============================] - 0s 2ms/step - loss: 0.0377 - val_loss: 0.0162\n","Epoch 12/20\n","192/192 [==============================] - 0s 2ms/step - loss: 0.0377 - val_loss: 0.0162\n","Epoch 13/20\n","192/192 [==============================] - 0s 2ms/step - loss: 0.0377 - val_loss: 0.0162\n","Epoch 14/20\n","192/192 [==============================] - 0s 2ms/step - loss: 0.0377 - val_loss: 0.0162\n","Epoch 15/20\n","192/192 [==============================] - 0s 2ms/step - loss: 0.0377 - val_loss: 0.0162\n","Epoch 16/20\n","192/192 [==============================] - 0s 2ms/step - loss: 0.0377 - val_loss: 0.0162\n","Epoch 17/20\n","192/192 [==============================] - 0s 2ms/step - loss: 0.0377 - val_loss: 0.0162\n","Epoch 18/20\n","192/192 [==============================] - 0s 2ms/step - loss: 0.0377 - val_loss: 0.0162\n","Epoch 19/20\n","192/192 [==============================] - 0s 2ms/step - loss: 0.0377 - val_loss: 0.0162\n","Epoch 20/20\n","192/192 [==============================] - 0s 2ms/step - loss: 0.0376 - val_loss: 0.0162\n","Fitting 3/30 model\n","Epoch 1/20\n","192/192 [==============================] - 1s 7ms/step - loss: 0.0528 - val_loss: 0.0164\n","Epoch 2/20\n","192/192 [==============================] - 1s 4ms/step - loss: 0.0432 - val_loss: 0.0160\n","Epoch 3/20\n","192/192 [==============================] - 1s 4ms/step - loss: 0.0394 - val_loss: 0.0162\n","Epoch 4/20\n","192/192 [==============================] - 1s 3ms/step - loss: 0.0418 - val_loss: 0.0161\n","Epoch 5/20\n","192/192 [==============================] - 1s 3ms/step - loss: 0.0401 - val_loss: 0.0162\n","Epoch 6/20\n","192/192 [==============================] - 1s 3ms/step - loss: 0.0399 - val_loss: 0.0161\n","Epoch 7/20\n","192/192 [==============================] - 1s 4ms/step - loss: 0.0413 - val_loss: 0.0161\n","Epoch 8/20\n","192/192 [==============================] - 1s 3ms/step - loss: 0.0396 - val_loss: 0.0162\n","Epoch 9/20\n","192/192 [==============================] - 1s 4ms/step - loss: 0.0404 - val_loss: 0.0165\n","Epoch 10/20\n","192/192 [==============================] - 1s 3ms/step - loss: 0.0392 - val_loss: 0.0163\n","Epoch 11/20\n","192/192 [==============================] - 1s 3ms/step - loss: 0.0404 - val_loss: 0.0163\n","Epoch 12/20\n","192/192 [==============================] - 1s 3ms/step - loss: 0.0392 - val_loss: 0.0164\n","Epoch 13/20\n","192/192 [==============================] - 1s 4ms/step - loss: 0.0400 - val_loss: 0.0162\n","Epoch 14/20\n","192/192 [==============================] - 1s 4ms/step - loss: 0.0407 - val_loss: 0.0162\n","Epoch 15/20\n","192/192 [==============================] - 1s 4ms/step - loss: 0.0402 - val_loss: 0.0163\n","Epoch 16/20\n","192/192 [==============================] - 1s 4ms/step - loss: 0.0388 - val_loss: 0.0167\n","Epoch 17/20\n","192/192 [==============================] - 1s 4ms/step - loss: 0.0403 - val_loss: 0.0167\n","Epoch 18/20\n","192/192 [==============================] - 1s 4ms/step - loss: 0.0391 - val_loss: 0.0165\n","Epoch 19/20\n","192/192 [==============================] - 1s 4ms/step - loss: 0.0387 - val_loss: 0.0166\n","Epoch 20/20\n","192/192 [==============================] - 1s 4ms/step - loss: 0.0382 - val_loss: 0.0163\n","Fitting 4/30 model\n","Epoch 1/20\n","192/192 [==============================] - 1s 7ms/step - loss: 0.0633 - val_loss: 0.0158\n","Epoch 2/20\n","192/192 [==============================] - 1s 4ms/step - loss: 0.0393 - val_loss: 0.0163\n","Epoch 3/20\n","192/192 [==============================] - 1s 4ms/step - loss: 0.0378 - val_loss: 0.0163\n","Epoch 4/20\n","192/192 [==============================] - 1s 4ms/step - loss: 0.0378 - val_loss: 0.0164\n","Epoch 5/20\n","192/192 [==============================] - 1s 4ms/step - loss: 0.0377 - val_loss: 0.0164\n","Epoch 6/20\n","192/192 [==============================] - 1s 4ms/step - loss: 0.0377 - val_loss: 0.0164\n","Epoch 7/20\n","192/192 [==============================] - 1s 4ms/step - loss: 0.0377 - val_loss: 0.0164\n","Epoch 8/20\n","192/192 [==============================] - 1s 4ms/step - loss: 0.0377 - val_loss: 0.0164\n","Epoch 9/20\n","192/192 [==============================] - 1s 4ms/step - loss: 0.0377 - val_loss: 0.0164\n","Epoch 10/20\n","192/192 [==============================] - 1s 4ms/step - loss: 0.0377 - val_loss: 0.0165\n","Epoch 11/20\n","192/192 [==============================] - 1s 4ms/step - loss: 0.0377 - val_loss: 0.0165\n","Epoch 12/20\n","192/192 [==============================] - 1s 4ms/step - loss: 0.0377 - val_loss: 0.0165\n","Epoch 13/20\n","192/192 [==============================] - 1s 4ms/step - loss: 0.0377 - val_loss: 0.0165\n","Epoch 14/20\n","192/192 [==============================] - 1s 4ms/step - loss: 0.0377 - val_loss: 0.0165\n","Epoch 15/20\n","192/192 [==============================] - 1s 4ms/step - loss: 0.0376 - val_loss: 0.0165\n","Epoch 16/20\n","192/192 [==============================] - 1s 4ms/step - loss: 0.0376 - val_loss: 0.0165\n","Epoch 17/20\n","192/192 [==============================] - 1s 3ms/step - loss: 0.0376 - val_loss: 0.0165\n","Epoch 18/20\n","192/192 [==============================] - 1s 4ms/step - loss: 0.0376 - val_loss: 0.0165\n","Epoch 19/20\n","192/192 [==============================] - 1s 4ms/step - loss: 0.0376 - val_loss: 0.0165\n","Epoch 20/20\n","192/192 [==============================] - 1s 4ms/step - loss: 0.0376 - val_loss: 0.0165\n","Fitting 5/30 model\n","Epoch 1/20\n","192/192 [==============================] - 2s 10ms/step - loss: 0.0575 - val_loss: 0.0161\n","Epoch 2/20\n","192/192 [==============================] - 1s 5ms/step - loss: 0.0398 - val_loss: 0.0165\n","Epoch 3/20\n","192/192 [==============================] - 1s 5ms/step - loss: 0.0403 - val_loss: 0.0164\n","Epoch 4/20\n","192/192 [==============================] - 1s 5ms/step - loss: 0.0408 - val_loss: 0.0169\n","Epoch 5/20\n","192/192 [==============================] - 1s 5ms/step - loss: 0.0384 - val_loss: 0.0168\n","Epoch 6/20\n","192/192 [==============================] - 1s 5ms/step - loss: 0.0400 - val_loss: 0.0161\n","Epoch 7/20\n","192/192 [==============================] - 1s 5ms/step - loss: 0.0411 - val_loss: 0.0165\n","Epoch 8/20\n","192/192 [==============================] - 1s 5ms/step - loss: 0.0414 - val_loss: 0.0169\n","Epoch 9/20\n","192/192 [==============================] - 1s 5ms/step - loss: 0.0410 - val_loss: 0.0167\n","Epoch 10/20\n","192/192 [==============================] - 1s 5ms/step - loss: 0.0384 - val_loss: 0.0172\n","Epoch 11/20\n","192/192 [==============================] - 1s 5ms/step - loss: 0.0401 - val_loss: 0.0170\n","Epoch 12/20\n","192/192 [==============================] - 1s 5ms/step - loss: 0.0398 - val_loss: 0.0171\n","Epoch 13/20\n","192/192 [==============================] - 1s 5ms/step - loss: 0.0407 - val_loss: 0.0164\n","Epoch 14/20\n","192/192 [==============================] - 1s 5ms/step - loss: 0.0393 - val_loss: 0.0167\n","Epoch 15/20\n","192/192 [==============================] - 1s 5ms/step - loss: 0.0401 - val_loss: 0.0171\n","Epoch 16/20\n","192/192 [==============================] - 1s 5ms/step - loss: 0.0382 - val_loss: 0.0169\n","Epoch 17/20\n","192/192 [==============================] - 1s 5ms/step - loss: 0.0390 - val_loss: 0.0167\n","Epoch 18/20\n","192/192 [==============================] - 1s 5ms/step - loss: 0.0410 - val_loss: 0.0167\n","Epoch 19/20\n","192/192 [==============================] - 1s 5ms/step - loss: 0.0401 - val_loss: 0.0168\n","Epoch 20/20\n","192/192 [==============================] - 1s 5ms/step - loss: 0.0419 - val_loss: 0.0166\n","Fitting 6/30 model\n","Epoch 1/20\n","192/192 [==============================] - 2s 10ms/step - loss: 0.0541 - val_loss: 0.0164\n","Epoch 2/20\n","192/192 [==============================] - 1s 6ms/step - loss: 0.0389 - val_loss: 0.0165\n","Epoch 3/20\n","192/192 [==============================] - 1s 5ms/step - loss: 0.0386 - val_loss: 0.0165\n","Epoch 4/20\n","192/192 [==============================] - 1s 5ms/step - loss: 0.0385 - val_loss: 0.0166\n","Epoch 5/20\n","192/192 [==============================] - 1s 5ms/step - loss: 0.0385 - val_loss: 0.0166\n","Epoch 6/20\n","192/192 [==============================] - 1s 5ms/step - loss: 0.0384 - val_loss: 0.0166\n","Epoch 7/20\n","192/192 [==============================] - 1s 5ms/step - loss: 0.0383 - val_loss: 0.0166\n","Epoch 8/20\n","192/192 [==============================] - 1s 5ms/step - loss: 0.0383 - val_loss: 0.0166\n","Epoch 9/20\n","192/192 [==============================] - 1s 5ms/step - loss: 0.0382 - val_loss: 0.0166\n","Epoch 10/20\n","192/192 [==============================] - 1s 5ms/step - loss: 0.0382 - val_loss: 0.0166\n","Epoch 11/20\n","192/192 [==============================] - 1s 6ms/step - loss: 0.0381 - val_loss: 0.0166\n","Epoch 12/20\n","192/192 [==============================] - 1s 5ms/step - loss: 0.0381 - val_loss: 0.0166\n","Epoch 13/20\n","192/192 [==============================] - 1s 5ms/step - loss: 0.0381 - val_loss: 0.0166\n","Epoch 14/20\n","192/192 [==============================] - 1s 5ms/step - loss: 0.0380 - val_loss: 0.0166\n","Epoch 15/20\n","192/192 [==============================] - 1s 5ms/step - loss: 0.0380 - val_loss: 0.0166\n","Epoch 16/20\n","192/192 [==============================] - 1s 5ms/step - loss: 0.0380 - val_loss: 0.0166\n","Epoch 17/20\n","192/192 [==============================] - 1s 5ms/step - loss: 0.0379 - val_loss: 0.0166\n","Epoch 18/20\n","192/192 [==============================] - 1s 6ms/step - loss: 0.0379 - val_loss: 0.0166\n","Epoch 19/20\n","192/192 [==============================] - 1s 5ms/step - loss: 0.0379 - val_loss: 0.0166\n","Epoch 20/20\n","192/192 [==============================] - 1s 5ms/step - loss: 0.0379 - val_loss: 0.0166\n","Fitting 7/30 model\n","Epoch 1/20\n","187/187 [==============================] - 1s 6ms/step - loss: 0.0746 - val_loss: 0.0174\n","Epoch 2/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0469 - val_loss: 0.0151\n","Epoch 3/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0415 - val_loss: 0.0155\n","Epoch 4/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0372 - val_loss: 0.0159\n","Epoch 5/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0380 - val_loss: 0.0160\n","Epoch 6/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0386 - val_loss: 0.0158\n","Epoch 7/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0399 - val_loss: 0.0158\n","Epoch 8/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0366 - val_loss: 0.0160\n","Epoch 9/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0388 - val_loss: 0.0159\n","Epoch 10/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0395 - val_loss: 0.0159\n","Epoch 11/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0370 - val_loss: 0.0160\n","Epoch 12/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0398 - val_loss: 0.0159\n","Epoch 13/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0373 - val_loss: 0.0162\n","Epoch 14/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0374 - val_loss: 0.0160\n","Epoch 15/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0374 - val_loss: 0.0158\n","Epoch 16/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0391 - val_loss: 0.0157\n","Epoch 17/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0359 - val_loss: 0.0162\n","Epoch 18/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0376 - val_loss: 0.0161\n","Epoch 19/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0399 - val_loss: 0.0160\n","Epoch 20/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0389 - val_loss: 0.0159\n","Fitting 8/30 model\n","Epoch 1/20\n","187/187 [==============================] - 1s 6ms/step - loss: 0.0622 - val_loss: 0.0153\n","Epoch 2/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0400 - val_loss: 0.0154\n","Epoch 3/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0374 - val_loss: 0.0155\n","Epoch 4/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0373 - val_loss: 0.0156\n","Epoch 5/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0373 - val_loss: 0.0156\n","Epoch 6/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0373 - val_loss: 0.0157\n","Epoch 7/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0373 - val_loss: 0.0157\n","Epoch 8/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0373 - val_loss: 0.0157\n","Epoch 9/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0373 - val_loss: 0.0157\n","Epoch 10/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0373 - val_loss: 0.0158\n","Epoch 11/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0373 - val_loss: 0.0158\n","Epoch 12/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0373 - val_loss: 0.0158\n","Epoch 13/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0373 - val_loss: 0.0158\n","Epoch 14/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0372 - val_loss: 0.0158\n","Epoch 15/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0372 - val_loss: 0.0158\n","Epoch 16/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0372 - val_loss: 0.0158\n","Epoch 17/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0372 - val_loss: 0.0158\n","Epoch 18/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0372 - val_loss: 0.0158\n","Epoch 19/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0372 - val_loss: 0.0159\n","Epoch 20/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0372 - val_loss: 0.0159\n","Fitting 9/30 model\n","Epoch 1/20\n","187/187 [==============================] - 2s 10ms/step - loss: 0.0599 - val_loss: 0.0148\n","Epoch 2/20\n","187/187 [==============================] - 1s 7ms/step - loss: 0.0413 - val_loss: 0.0153\n","Epoch 3/20\n","187/187 [==============================] - 1s 7ms/step - loss: 0.0390 - val_loss: 0.0152\n","Epoch 4/20\n","187/187 [==============================] - 1s 7ms/step - loss: 0.0395 - val_loss: 0.0157\n","Epoch 5/20\n","187/187 [==============================] - 1s 7ms/step - loss: 0.0406 - val_loss: 0.0152\n","Epoch 6/20\n","187/187 [==============================] - 1s 7ms/step - loss: 0.0388 - val_loss: 0.0154\n","Epoch 7/20\n","187/187 [==============================] - 1s 7ms/step - loss: 0.0394 - val_loss: 0.0155\n","Epoch 8/20\n","187/187 [==============================] - 1s 7ms/step - loss: 0.0397 - val_loss: 0.0154\n","Epoch 9/20\n","187/187 [==============================] - 1s 7ms/step - loss: 0.0414 - val_loss: 0.0151\n","Epoch 10/20\n","187/187 [==============================] - 1s 7ms/step - loss: 0.0384 - val_loss: 0.0160\n","Epoch 11/20\n","187/187 [==============================] - 1s 7ms/step - loss: 0.0385 - val_loss: 0.0155\n","Epoch 12/20\n","187/187 [==============================] - 1s 7ms/step - loss: 0.0387 - val_loss: 0.0153\n","Epoch 13/20\n","187/187 [==============================] - 1s 7ms/step - loss: 0.0395 - val_loss: 0.0159\n","Epoch 14/20\n","187/187 [==============================] - 1s 7ms/step - loss: 0.0384 - val_loss: 0.0159\n","Epoch 15/20\n","187/187 [==============================] - 1s 7ms/step - loss: 0.0388 - val_loss: 0.0157\n","Epoch 16/20\n","187/187 [==============================] - 1s 7ms/step - loss: 0.0409 - val_loss: 0.0155\n","Epoch 17/20\n","187/187 [==============================] - 1s 7ms/step - loss: 0.0395 - val_loss: 0.0154\n","Epoch 18/20\n","187/187 [==============================] - 1s 7ms/step - loss: 0.0404 - val_loss: 0.0154\n","Epoch 19/20\n","187/187 [==============================] - 1s 7ms/step - loss: 0.0390 - val_loss: 0.0155\n","Epoch 20/20\n","187/187 [==============================] - 1s 7ms/step - loss: 0.0392 - val_loss: 0.0154\n","Fitting 10/30 model\n","Epoch 1/20\n","187/187 [==============================] - 2s 11ms/step - loss: 0.0546 - val_loss: 0.0149\n","Epoch 2/20\n","187/187 [==============================] - 1s 7ms/step - loss: 0.0378 - val_loss: 0.0153\n","Epoch 3/20\n","187/187 [==============================] - 1s 7ms/step - loss: 0.0374 - val_loss: 0.0153\n","Epoch 4/20\n","187/187 [==============================] - 1s 7ms/step - loss: 0.0374 - val_loss: 0.0154\n","Epoch 5/20\n","187/187 [==============================] - 1s 7ms/step - loss: 0.0374 - val_loss: 0.0154\n","Epoch 6/20\n","187/187 [==============================] - 1s 7ms/step - loss: 0.0373 - val_loss: 0.0154\n","Epoch 7/20\n","187/187 [==============================] - 1s 7ms/step - loss: 0.0373 - val_loss: 0.0154\n","Epoch 8/20\n","187/187 [==============================] - 1s 7ms/step - loss: 0.0373 - val_loss: 0.0154\n","Epoch 9/20\n","187/187 [==============================] - 1s 7ms/step - loss: 0.0373 - val_loss: 0.0155\n","Epoch 10/20\n","187/187 [==============================] - 1s 7ms/step - loss: 0.0373 - val_loss: 0.0155\n","Epoch 11/20\n","187/187 [==============================] - 1s 7ms/step - loss: 0.0373 - val_loss: 0.0155\n","Epoch 12/20\n","187/187 [==============================] - 1s 7ms/step - loss: 0.0373 - val_loss: 0.0155\n","Epoch 13/20\n","187/187 [==============================] - 1s 7ms/step - loss: 0.0373 - val_loss: 0.0155\n","Epoch 14/20\n","187/187 [==============================] - 1s 7ms/step - loss: 0.0373 - val_loss: 0.0155\n","Epoch 15/20\n","187/187 [==============================] - 2s 10ms/step - loss: 0.0373 - val_loss: 0.0156\n","Epoch 16/20\n","187/187 [==============================] - 2s 11ms/step - loss: 0.0373 - val_loss: 0.0156\n","Epoch 17/20\n","187/187 [==============================] - 2s 11ms/step - loss: 0.0373 - val_loss: 0.0156\n","Epoch 18/20\n","187/187 [==============================] - 2s 12ms/step - loss: 0.0373 - val_loss: 0.0156\n","Epoch 19/20\n","187/187 [==============================] - 2s 10ms/step - loss: 0.0373 - val_loss: 0.0156\n","Epoch 20/20\n","187/187 [==============================] - 1s 7ms/step - loss: 0.0373 - val_loss: 0.0156\n","Fitting 11/30 model\n","Epoch 1/20\n","187/187 [==============================] - 3s 14ms/step - loss: 0.0550 - val_loss: 0.0149\n","Epoch 2/20\n","187/187 [==============================] - 2s 10ms/step - loss: 0.0421 - val_loss: 0.0152\n","Epoch 3/20\n","187/187 [==============================] - 2s 10ms/step - loss: 0.0370 - val_loss: 0.0156\n","Epoch 4/20\n","187/187 [==============================] - 2s 10ms/step - loss: 0.0401 - val_loss: 0.0151\n","Epoch 5/20\n","187/187 [==============================] - 2s 10ms/step - loss: 0.0396 - val_loss: 0.0154\n","Epoch 6/20\n","187/187 [==============================] - 2s 10ms/step - loss: 0.0404 - val_loss: 0.0152\n","Epoch 7/20\n","187/187 [==============================] - 2s 10ms/step - loss: 0.0398 - val_loss: 0.0156\n","Epoch 8/20\n","187/187 [==============================] - 2s 10ms/step - loss: 0.0381 - val_loss: 0.0156\n","Epoch 9/20\n","187/187 [==============================] - 2s 10ms/step - loss: 0.0399 - val_loss: 0.0154\n","Epoch 10/20\n","187/187 [==============================] - 2s 10ms/step - loss: 0.0395 - val_loss: 0.0152\n","Epoch 11/20\n","187/187 [==============================] - 2s 10ms/step - loss: 0.0403 - val_loss: 0.0154\n","Epoch 12/20\n","187/187 [==============================] - 2s 10ms/step - loss: 0.0384 - val_loss: 0.0158\n","Epoch 13/20\n","187/187 [==============================] - 2s 10ms/step - loss: 0.0396 - val_loss: 0.0153\n","Epoch 14/20\n","187/187 [==============================] - 2s 10ms/step - loss: 0.0387 - val_loss: 0.0154\n","Epoch 15/20\n","187/187 [==============================] - 2s 10ms/step - loss: 0.0401 - val_loss: 0.0154\n","Epoch 16/20\n","187/187 [==============================] - 2s 10ms/step - loss: 0.0382 - val_loss: 0.0155\n","Epoch 17/20\n","187/187 [==============================] - 2s 10ms/step - loss: 0.0395 - val_loss: 0.0153\n","Epoch 18/20\n","187/187 [==============================] - 2s 10ms/step - loss: 0.0383 - val_loss: 0.0154\n","Epoch 19/20\n","187/187 [==============================] - 2s 10ms/step - loss: 0.0397 - val_loss: 0.0153\n","Epoch 20/20\n","187/187 [==============================] - 2s 10ms/step - loss: 0.0406 - val_loss: 0.0152\n","Fitting 12/30 model\n","Epoch 1/20\n","187/187 [==============================] - 3s 19ms/step - loss: 0.0538 - val_loss: 0.0151\n","Epoch 2/20\n","187/187 [==============================] - 2s 10ms/step - loss: 0.0378 - val_loss: 0.0152\n","Epoch 3/20\n","187/187 [==============================] - 2s 10ms/step - loss: 0.0377 - val_loss: 0.0152\n","Epoch 4/20\n","187/187 [==============================] - 2s 10ms/step - loss: 0.0376 - val_loss: 0.0153\n","Epoch 5/20\n","187/187 [==============================] - 2s 10ms/step - loss: 0.0376 - val_loss: 0.0153\n","Epoch 6/20\n","187/187 [==============================] - 2s 10ms/step - loss: 0.0376 - val_loss: 0.0153\n","Epoch 7/20\n","187/187 [==============================] - 2s 10ms/step - loss: 0.0376 - val_loss: 0.0153\n","Epoch 8/20\n","187/187 [==============================] - 2s 10ms/step - loss: 0.0376 - val_loss: 0.0153\n","Epoch 9/20\n","187/187 [==============================] - 2s 10ms/step - loss: 0.0376 - val_loss: 0.0154\n","Epoch 10/20\n","187/187 [==============================] - 2s 10ms/step - loss: 0.0376 - val_loss: 0.0154\n","Epoch 11/20\n","187/187 [==============================] - 2s 10ms/step - loss: 0.0376 - val_loss: 0.0154\n","Epoch 12/20\n","187/187 [==============================] - 2s 10ms/step - loss: 0.0376 - val_loss: 0.0154\n","Epoch 13/20\n","187/187 [==============================] - 2s 10ms/step - loss: 0.0376 - val_loss: 0.0154\n","Epoch 14/20\n","187/187 [==============================] - 2s 10ms/step - loss: 0.0376 - val_loss: 0.0154\n","Epoch 15/20\n","187/187 [==============================] - 2s 10ms/step - loss: 0.0376 - val_loss: 0.0155\n","Epoch 16/20\n","187/187 [==============================] - 2s 10ms/step - loss: 0.0376 - val_loss: 0.0155\n","Epoch 17/20\n","187/187 [==============================] - 2s 10ms/step - loss: 0.0376 - val_loss: 0.0155\n","Epoch 18/20\n","187/187 [==============================] - 2s 10ms/step - loss: 0.0376 - val_loss: 0.0155\n","Epoch 19/20\n","187/187 [==============================] - 2s 10ms/step - loss: 0.0376 - val_loss: 0.0155\n","Epoch 20/20\n","187/187 [==============================] - 2s 10ms/step - loss: 0.0376 - val_loss: 0.0155\n","Fitting 13/30 model\n","Epoch 1/20\n","183/183 [==============================] - 1s 7ms/step - loss: 0.0738 - val_loss: 0.0173\n","Epoch 2/20\n","183/183 [==============================] - 1s 5ms/step - loss: 0.0437 - val_loss: 0.0151\n","Epoch 3/20\n","183/183 [==============================] - 1s 5ms/step - loss: 0.0360 - val_loss: 0.0160\n","Epoch 4/20\n","183/183 [==============================] - 1s 5ms/step - loss: 0.0354 - val_loss: 0.0159\n","Epoch 5/20\n","183/183 [==============================] - 1s 6ms/step - loss: 0.0345 - val_loss: 0.0157\n","Epoch 6/20\n","183/183 [==============================] - 1s 5ms/step - loss: 0.0363 - val_loss: 0.0158\n","Epoch 7/20\n","183/183 [==============================] - 1s 5ms/step - loss: 0.0356 - val_loss: 0.0157\n","Epoch 8/20\n","183/183 [==============================] - 1s 5ms/step - loss: 0.0364 - val_loss: 0.0159\n","Epoch 9/20\n","183/183 [==============================] - 1s 5ms/step - loss: 0.0346 - val_loss: 0.0158\n","Epoch 10/20\n","183/183 [==============================] - 1s 5ms/step - loss: 0.0358 - val_loss: 0.0158\n","Epoch 11/20\n","183/183 [==============================] - 1s 5ms/step - loss: 0.0351 - val_loss: 0.0160\n","Epoch 12/20\n","183/183 [==============================] - 1s 5ms/step - loss: 0.0351 - val_loss: 0.0161\n","Epoch 13/20\n","183/183 [==============================] - 1s 5ms/step - loss: 0.0335 - val_loss: 0.0162\n","Epoch 14/20\n","183/183 [==============================] - 1s 5ms/step - loss: 0.0344 - val_loss: 0.0161\n","Epoch 15/20\n","183/183 [==============================] - 1s 6ms/step - loss: 0.0346 - val_loss: 0.0161\n","Epoch 16/20\n","183/183 [==============================] - 1s 5ms/step - loss: 0.0340 - val_loss: 0.0163\n","Epoch 17/20\n","183/183 [==============================] - 1s 5ms/step - loss: 0.0351 - val_loss: 0.0163\n","Epoch 18/20\n","183/183 [==============================] - 1s 5ms/step - loss: 0.0356 - val_loss: 0.0160\n","Epoch 19/20\n","183/183 [==============================] - 1s 5ms/step - loss: 0.0346 - val_loss: 0.0160\n","Epoch 20/20\n","183/183 [==============================] - 1s 6ms/step - loss: 0.0355 - val_loss: 0.0159\n","Fitting 14/30 model\n","Epoch 1/20\n","183/183 [==============================] - 1s 7ms/step - loss: 0.0565 - val_loss: 0.0147\n","Epoch 2/20\n","183/183 [==============================] - 1s 5ms/step - loss: 0.0356 - val_loss: 0.0154\n","Epoch 3/20\n","183/183 [==============================] - 1s 5ms/step - loss: 0.0340 - val_loss: 0.0155\n","Epoch 4/20\n","183/183 [==============================] - 1s 6ms/step - loss: 0.0339 - val_loss: 0.0155\n","Epoch 5/20\n","183/183 [==============================] - 1s 5ms/step - loss: 0.0339 - val_loss: 0.0156\n","Epoch 6/20\n","183/183 [==============================] - 1s 5ms/step - loss: 0.0339 - val_loss: 0.0156\n","Epoch 7/20\n","183/183 [==============================] - 1s 5ms/step - loss: 0.0339 - val_loss: 0.0156\n","Epoch 8/20\n","183/183 [==============================] - 1s 5ms/step - loss: 0.0339 - val_loss: 0.0156\n","Epoch 9/20\n","183/183 [==============================] - 1s 5ms/step - loss: 0.0338 - val_loss: 0.0157\n","Epoch 10/20\n","183/183 [==============================] - 1s 5ms/step - loss: 0.0338 - val_loss: 0.0157\n","Epoch 11/20\n","183/183 [==============================] - 1s 5ms/step - loss: 0.0338 - val_loss: 0.0157\n","Epoch 12/20\n","183/183 [==============================] - 1s 5ms/step - loss: 0.0338 - val_loss: 0.0157\n","Epoch 13/20\n","183/183 [==============================] - 1s 5ms/step - loss: 0.0338 - val_loss: 0.0157\n","Epoch 14/20\n","183/183 [==============================] - 1s 5ms/step - loss: 0.0338 - val_loss: 0.0157\n","Epoch 15/20\n","183/183 [==============================] - 1s 5ms/step - loss: 0.0338 - val_loss: 0.0157\n","Epoch 16/20\n","183/183 [==============================] - 1s 5ms/step - loss: 0.0338 - val_loss: 0.0157\n","Epoch 17/20\n","183/183 [==============================] - 1s 5ms/step - loss: 0.0338 - val_loss: 0.0157\n","Epoch 18/20\n","183/183 [==============================] - 1s 5ms/step - loss: 0.0338 - val_loss: 0.0157\n","Epoch 19/20\n","183/183 [==============================] - 1s 5ms/step - loss: 0.0338 - val_loss: 0.0157\n","Epoch 20/20\n","183/183 [==============================] - 1s 5ms/step - loss: 0.0338 - val_loss: 0.0157\n","Fitting 15/30 model\n","Epoch 1/20\n","183/183 [==============================] - 2s 13ms/step - loss: 0.0638 - val_loss: 0.0147\n","Epoch 2/20\n","183/183 [==============================] - 2s 9ms/step - loss: 0.0388 - val_loss: 0.0153\n","Epoch 3/20\n","183/183 [==============================] - 2s 9ms/step - loss: 0.0361 - val_loss: 0.0153\n","Epoch 4/20\n","183/183 [==============================] - 2s 9ms/step - loss: 0.0384 - val_loss: 0.0154\n","Epoch 5/20\n","183/183 [==============================] - 2s 9ms/step - loss: 0.0376 - val_loss: 0.0152\n","Epoch 6/20\n","183/183 [==============================] - 2s 9ms/step - loss: 0.0350 - val_loss: 0.0154\n","Epoch 7/20\n","183/183 [==============================] - 2s 9ms/step - loss: 0.0362 - val_loss: 0.0149\n","Epoch 8/20\n","183/183 [==============================] - 2s 10ms/step - loss: 0.0348 - val_loss: 0.0154\n","Epoch 9/20\n","183/183 [==============================] - 2s 9ms/step - loss: 0.0365 - val_loss: 0.0154\n","Epoch 10/20\n","183/183 [==============================] - 2s 9ms/step - loss: 0.0352 - val_loss: 0.0157\n","Epoch 11/20\n","183/183 [==============================] - 2s 9ms/step - loss: 0.0350 - val_loss: 0.0153\n","Epoch 12/20\n","183/183 [==============================] - 2s 9ms/step - loss: 0.0354 - val_loss: 0.0155\n","Epoch 13/20\n","183/183 [==============================] - 2s 9ms/step - loss: 0.0331 - val_loss: 0.0157\n","Epoch 14/20\n","183/183 [==============================] - 2s 9ms/step - loss: 0.0367 - val_loss: 0.0151\n","Epoch 15/20\n","183/183 [==============================] - 2s 9ms/step - loss: 0.0367 - val_loss: 0.0153\n","Epoch 16/20\n","183/183 [==============================] - 2s 9ms/step - loss: 0.0357 - val_loss: 0.0156\n","Epoch 17/20\n","183/183 [==============================] - 2s 9ms/step - loss: 0.0365 - val_loss: 0.0155\n","Epoch 18/20\n","183/183 [==============================] - 2s 9ms/step - loss: 0.0353 - val_loss: 0.0155\n","Epoch 19/20\n","183/183 [==============================] - 2s 10ms/step - loss: 0.0365 - val_loss: 0.0153\n","Epoch 20/20\n","183/183 [==============================] - 2s 9ms/step - loss: 0.0356 - val_loss: 0.0153\n","Fitting 16/30 model\n","Epoch 1/20\n","183/183 [==============================] - 2s 13ms/step - loss: 0.0526 - val_loss: 0.0147\n","Epoch 2/20\n","183/183 [==============================] - 2s 9ms/step - loss: 0.0343 - val_loss: 0.0151\n","Epoch 3/20\n","183/183 [==============================] - 2s 9ms/step - loss: 0.0340 - val_loss: 0.0152\n","Epoch 4/20\n","183/183 [==============================] - 2s 9ms/step - loss: 0.0340 - val_loss: 0.0152\n","Epoch 5/20\n","183/183 [==============================] - 2s 9ms/step - loss: 0.0340 - val_loss: 0.0153\n","Epoch 6/20\n","183/183 [==============================] - 2s 9ms/step - loss: 0.0340 - val_loss: 0.0153\n","Epoch 7/20\n","183/183 [==============================] - 2s 9ms/step - loss: 0.0340 - val_loss: 0.0153\n","Epoch 8/20\n","183/183 [==============================] - 2s 9ms/step - loss: 0.0339 - val_loss: 0.0153\n","Epoch 9/20\n","183/183 [==============================] - 2s 9ms/step - loss: 0.0339 - val_loss: 0.0154\n","Epoch 10/20\n","183/183 [==============================] - 2s 9ms/step - loss: 0.0339 - val_loss: 0.0154\n","Epoch 11/20\n","183/183 [==============================] - 2s 9ms/step - loss: 0.0339 - val_loss: 0.0154\n","Epoch 12/20\n","183/183 [==============================] - 2s 9ms/step - loss: 0.0339 - val_loss: 0.0154\n","Epoch 13/20\n","183/183 [==============================] - 2s 9ms/step - loss: 0.0339 - val_loss: 0.0154\n","Epoch 14/20\n","183/183 [==============================] - 2s 9ms/step - loss: 0.0339 - val_loss: 0.0155\n","Epoch 15/20\n","183/183 [==============================] - 2s 9ms/step - loss: 0.0339 - val_loss: 0.0155\n","Epoch 16/20\n","183/183 [==============================] - 2s 9ms/step - loss: 0.0339 - val_loss: 0.0155\n","Epoch 17/20\n","183/183 [==============================] - 2s 9ms/step - loss: 0.0339 - val_loss: 0.0155\n","Epoch 18/20\n","183/183 [==============================] - 2s 9ms/step - loss: 0.0339 - val_loss: 0.0155\n","Epoch 19/20\n","183/183 [==============================] - 2s 9ms/step - loss: 0.0339 - val_loss: 0.0155\n","Epoch 20/20\n","183/183 [==============================] - 2s 9ms/step - loss: 0.0339 - val_loss: 0.0155\n","Fitting 17/30 model\n","Epoch 1/20\n","183/183 [==============================] - 3s 18ms/step - loss: 0.0528 - val_loss: 0.0147\n","Epoch 2/20\n","183/183 [==============================] - 2s 13ms/step - loss: 0.0380 - val_loss: 0.0152\n","Epoch 3/20\n","183/183 [==============================] - 2s 13ms/step - loss: 0.0387 - val_loss: 0.0150\n","Epoch 4/20\n","183/183 [==============================] - 2s 12ms/step - loss: 0.0374 - val_loss: 0.0153\n","Epoch 5/20\n","183/183 [==============================] - 2s 13ms/step - loss: 0.0368 - val_loss: 0.0155\n","Epoch 6/20\n","183/183 [==============================] - 2s 13ms/step - loss: 0.0355 - val_loss: 0.0153\n","Epoch 7/20\n","183/183 [==============================] - 2s 13ms/step - loss: 0.0349 - val_loss: 0.0154\n","Epoch 8/20\n","183/183 [==============================] - 2s 13ms/step - loss: 0.0360 - val_loss: 0.0156\n","Epoch 9/20\n","183/183 [==============================] - 2s 13ms/step - loss: 0.0360 - val_loss: 0.0152\n","Epoch 10/20\n","183/183 [==============================] - 2s 13ms/step - loss: 0.0360 - val_loss: 0.0154\n","Epoch 11/20\n","183/183 [==============================] - 2s 13ms/step - loss: 0.0366 - val_loss: 0.0151\n","Epoch 12/20\n","183/183 [==============================] - 2s 13ms/step - loss: 0.0363 - val_loss: 0.0152\n","Epoch 13/20\n","183/183 [==============================] - 2s 13ms/step - loss: 0.0348 - val_loss: 0.0154\n","Epoch 14/20\n","183/183 [==============================] - 2s 13ms/step - loss: 0.0353 - val_loss: 0.0154\n","Epoch 15/20\n","183/183 [==============================] - 2s 13ms/step - loss: 0.0367 - val_loss: 0.0155\n","Epoch 16/20\n","183/183 [==============================] - 2s 13ms/step - loss: 0.0341 - val_loss: 0.0160\n","Epoch 17/20\n","183/183 [==============================] - 3s 14ms/step - loss: 0.0335 - val_loss: 0.0158\n","Epoch 18/20\n","183/183 [==============================] - 3s 14ms/step - loss: 0.0346 - val_loss: 0.0156\n","Epoch 19/20\n","183/183 [==============================] - 2s 13ms/step - loss: 0.0356 - val_loss: 0.0154\n","Epoch 20/20\n","183/183 [==============================] - 2s 13ms/step - loss: 0.0351 - val_loss: 0.0154\n","Fitting 18/30 model\n","Epoch 1/20\n","183/183 [==============================] - 3s 18ms/step - loss: 0.0444 - val_loss: 0.0151\n","Epoch 2/20\n","183/183 [==============================] - 2s 13ms/step - loss: 0.0343 - val_loss: 0.0151\n","Epoch 3/20\n","183/183 [==============================] - 2s 13ms/step - loss: 0.0342 - val_loss: 0.0151\n","Epoch 4/20\n","183/183 [==============================] - 2s 13ms/step - loss: 0.0342 - val_loss: 0.0152\n","Epoch 5/20\n","183/183 [==============================] - 2s 13ms/step - loss: 0.0342 - val_loss: 0.0152\n","Epoch 6/20\n","183/183 [==============================] - 2s 13ms/step - loss: 0.0342 - val_loss: 0.0152\n","Epoch 7/20\n","183/183 [==============================] - 2s 13ms/step - loss: 0.0342 - val_loss: 0.0152\n","Epoch 8/20\n","183/183 [==============================] - 2s 13ms/step - loss: 0.0342 - val_loss: 0.0152\n","Epoch 9/20\n","183/183 [==============================] - 2s 13ms/step - loss: 0.0342 - val_loss: 0.0153\n","Epoch 10/20\n","183/183 [==============================] - 2s 13ms/step - loss: 0.0342 - val_loss: 0.0153\n","Epoch 11/20\n","183/183 [==============================] - 2s 13ms/step - loss: 0.0342 - val_loss: 0.0153\n","Epoch 12/20\n","183/183 [==============================] - 2s 13ms/step - loss: 0.0342 - val_loss: 0.0153\n","Epoch 13/20\n","183/183 [==============================] - 3s 14ms/step - loss: 0.0342 - val_loss: 0.0153\n","Epoch 14/20\n","183/183 [==============================] - 2s 14ms/step - loss: 0.0342 - val_loss: 0.0154\n","Epoch 15/20\n","183/183 [==============================] - 2s 13ms/step - loss: 0.0342 - val_loss: 0.0154\n","Epoch 16/20\n","183/183 [==============================] - 2s 13ms/step - loss: 0.0342 - val_loss: 0.0154\n","Epoch 17/20\n","183/183 [==============================] - 2s 14ms/step - loss: 0.0342 - val_loss: 0.0154\n","Epoch 18/20\n","183/183 [==============================] - 2s 14ms/step - loss: 0.0342 - val_loss: 0.0154\n","Epoch 19/20\n","183/183 [==============================] - 3s 14ms/step - loss: 0.0342 - val_loss: 0.0155\n","Epoch 20/20\n","183/183 [==============================] - 2s 13ms/step - loss: 0.0341 - val_loss: 0.0155\n","Fitting 19/30 model\n","Epoch 1/20\n","171/171 [==============================] - 2s 11ms/step - loss: 0.0547 - val_loss: 0.0153\n","Epoch 2/20\n","171/171 [==============================] - 1s 9ms/step - loss: 0.0394 - val_loss: 0.0145\n","Epoch 3/20\n","171/171 [==============================] - 1s 8ms/step - loss: 0.0362 - val_loss: 0.0148\n","Epoch 4/20\n","171/171 [==============================] - 1s 9ms/step - loss: 0.0375 - val_loss: 0.0150\n","Epoch 5/20\n","171/171 [==============================] - 1s 8ms/step - loss: 0.0368 - val_loss: 0.0150\n","Epoch 6/20\n","171/171 [==============================] - 1s 8ms/step - loss: 0.0349 - val_loss: 0.0149\n","Epoch 7/20\n","171/171 [==============================] - 1s 8ms/step - loss: 0.0371 - val_loss: 0.0150\n","Epoch 8/20\n","171/171 [==============================] - 1s 8ms/step - loss: 0.0355 - val_loss: 0.0151\n","Epoch 9/20\n","171/171 [==============================] - 1s 9ms/step - loss: 0.0342 - val_loss: 0.0152\n","Epoch 10/20\n","171/171 [==============================] - 1s 8ms/step - loss: 0.0357 - val_loss: 0.0153\n","Epoch 11/20\n","171/171 [==============================] - 1s 8ms/step - loss: 0.0351 - val_loss: 0.0153\n","Epoch 12/20\n","171/171 [==============================] - 1s 8ms/step - loss: 0.0339 - val_loss: 0.0153\n","Epoch 13/20\n","171/171 [==============================] - 1s 8ms/step - loss: 0.0359 - val_loss: 0.0152\n","Epoch 14/20\n","171/171 [==============================] - 1s 8ms/step - loss: 0.0341 - val_loss: 0.0151\n","Epoch 15/20\n","171/171 [==============================] - 1s 9ms/step - loss: 0.0363 - val_loss: 0.0151\n","Epoch 16/20\n","171/171 [==============================] - 1s 8ms/step - loss: 0.0334 - val_loss: 0.0153\n","Epoch 17/20\n","171/171 [==============================] - 1s 8ms/step - loss: 0.0352 - val_loss: 0.0151\n","Epoch 18/20\n","171/171 [==============================] - 1s 8ms/step - loss: 0.0350 - val_loss: 0.0153\n","Epoch 19/20\n","171/171 [==============================] - 1s 8ms/step - loss: 0.0335 - val_loss: 0.0153\n","Epoch 20/20\n","171/171 [==============================] - 2s 9ms/step - loss: 0.0347 - val_loss: 0.0153\n","Fitting 20/30 model\n","Epoch 1/20\n","171/171 [==============================] - 2s 10ms/step - loss: 0.0448 - val_loss: 0.0145\n","Epoch 2/20\n","171/171 [==============================] - 1s 8ms/step - loss: 0.0346 - val_loss: 0.0149\n","Epoch 3/20\n","171/171 [==============================] - 1s 8ms/step - loss: 0.0339 - val_loss: 0.0149\n","Epoch 4/20\n","171/171 [==============================] - 1s 9ms/step - loss: 0.0339 - val_loss: 0.0150\n","Epoch 5/20\n","171/171 [==============================] - 1s 9ms/step - loss: 0.0339 - val_loss: 0.0150\n","Epoch 6/20\n","171/171 [==============================] - 1s 9ms/step - loss: 0.0339 - val_loss: 0.0150\n","Epoch 7/20\n","171/171 [==============================] - 1s 9ms/step - loss: 0.0339 - val_loss: 0.0150\n","Epoch 8/20\n","171/171 [==============================] - 1s 9ms/step - loss: 0.0339 - val_loss: 0.0150\n","Epoch 9/20\n","171/171 [==============================] - 1s 9ms/step - loss: 0.0339 - val_loss: 0.0150\n","Epoch 10/20\n","171/171 [==============================] - 1s 8ms/step - loss: 0.0339 - val_loss: 0.0151\n","Epoch 11/20\n","171/171 [==============================] - 1s 9ms/step - loss: 0.0339 - val_loss: 0.0151\n","Epoch 12/20\n","171/171 [==============================] - 1s 9ms/step - loss: 0.0339 - val_loss: 0.0151\n","Epoch 13/20\n","171/171 [==============================] - 1s 8ms/step - loss: 0.0339 - val_loss: 0.0151\n","Epoch 14/20\n","171/171 [==============================] - 1s 8ms/step - loss: 0.0339 - val_loss: 0.0151\n","Epoch 15/20\n","171/171 [==============================] - 1s 8ms/step - loss: 0.0339 - val_loss: 0.0151\n","Epoch 16/20\n","171/171 [==============================] - 1s 8ms/step - loss: 0.0339 - val_loss: 0.0151\n","Epoch 17/20\n","171/171 [==============================] - 1s 8ms/step - loss: 0.0339 - val_loss: 0.0151\n","Epoch 18/20\n","171/171 [==============================] - 1s 8ms/step - loss: 0.0339 - val_loss: 0.0151\n","Epoch 19/20\n","171/171 [==============================] - 1s 8ms/step - loss: 0.0339 - val_loss: 0.0151\n","Epoch 20/20\n","171/171 [==============================] - 1s 8ms/step - loss: 0.0339 - val_loss: 0.0151\n","Fitting 21/30 model\n","Epoch 1/20\n","171/171 [==============================] - 3s 20ms/step - loss: 0.0548 - val_loss: 0.0149\n","Epoch 2/20\n","171/171 [==============================] - 3s 16ms/step - loss: 0.0351 - val_loss: 0.0152\n","Epoch 3/20\n","171/171 [==============================] - 3s 17ms/step - loss: 0.0372 - val_loss: 0.0150\n","Epoch 4/20\n","171/171 [==============================] - 3s 16ms/step - loss: 0.0369 - val_loss: 0.0150\n","Epoch 5/20\n","171/171 [==============================] - 3s 16ms/step - loss: 0.0343 - val_loss: 0.0151\n","Epoch 6/20\n","171/171 [==============================] - 3s 16ms/step - loss: 0.0370 - val_loss: 0.0151\n","Epoch 7/20\n","171/171 [==============================] - 3s 16ms/step - loss: 0.0339 - val_loss: 0.0154\n","Epoch 8/20\n","171/171 [==============================] - 3s 16ms/step - loss: 0.0353 - val_loss: 0.0153\n","Epoch 9/20\n","171/171 [==============================] - 3s 16ms/step - loss: 0.0364 - val_loss: 0.0151\n","Epoch 10/20\n","171/171 [==============================] - 3s 16ms/step - loss: 0.0356 - val_loss: 0.0154\n","Epoch 11/20\n","171/171 [==============================] - 3s 16ms/step - loss: 0.0368 - val_loss: 0.0155\n","Epoch 12/20\n","171/171 [==============================] - 3s 16ms/step - loss: 0.0363 - val_loss: 0.0154\n","Epoch 13/20\n","171/171 [==============================] - 3s 16ms/step - loss: 0.0353 - val_loss: 0.0154\n","Epoch 14/20\n","171/171 [==============================] - 3s 16ms/step - loss: 0.0361 - val_loss: 0.0150\n","Epoch 15/20\n","171/171 [==============================] - 3s 17ms/step - loss: 0.0355 - val_loss: 0.0153\n","Epoch 16/20\n","171/171 [==============================] - 3s 16ms/step - loss: 0.0340 - val_loss: 0.0155\n","Epoch 17/20\n","171/171 [==============================] - 3s 17ms/step - loss: 0.0354 - val_loss: 0.0153\n","Epoch 18/20\n","171/171 [==============================] - 3s 16ms/step - loss: 0.0364 - val_loss: 0.0154\n","Epoch 19/20\n","171/171 [==============================] - 3s 16ms/step - loss: 0.0351 - val_loss: 0.0154\n","Epoch 20/20\n","171/171 [==============================] - 3s 17ms/step - loss: 0.0356 - val_loss: 0.0154\n","Fitting 22/30 model\n","Epoch 1/20\n","171/171 [==============================] - 3s 19ms/step - loss: 0.0518 - val_loss: 0.0147\n","Epoch 2/20\n","171/171 [==============================] - 3s 15ms/step - loss: 0.0349 - val_loss: 0.0150\n","Epoch 3/20\n","171/171 [==============================] - 3s 15ms/step - loss: 0.0347 - val_loss: 0.0150\n","Epoch 4/20\n","171/171 [==============================] - 3s 15ms/step - loss: 0.0346 - val_loss: 0.0150\n","Epoch 5/20\n","171/171 [==============================] - 3s 15ms/step - loss: 0.0346 - val_loss: 0.0151\n","Epoch 6/20\n","171/171 [==============================] - 3s 15ms/step - loss: 0.0346 - val_loss: 0.0151\n","Epoch 7/20\n","171/171 [==============================] - 3s 15ms/step - loss: 0.0345 - val_loss: 0.0151\n","Epoch 8/20\n","171/171 [==============================] - 3s 16ms/step - loss: 0.0345 - val_loss: 0.0151\n","Epoch 9/20\n","171/171 [==============================] - 3s 16ms/step - loss: 0.0345 - val_loss: 0.0152\n","Epoch 10/20\n","171/171 [==============================] - 3s 16ms/step - loss: 0.0345 - val_loss: 0.0152\n","Epoch 11/20\n","171/171 [==============================] - 3s 16ms/step - loss: 0.0344 - val_loss: 0.0152\n","Epoch 12/20\n","171/171 [==============================] - 3s 16ms/step - loss: 0.0344 - val_loss: 0.0152\n","Epoch 13/20\n","171/171 [==============================] - 3s 15ms/step - loss: 0.0344 - val_loss: 0.0152\n","Epoch 14/20\n","171/171 [==============================] - 3s 15ms/step - loss: 0.0344 - val_loss: 0.0152\n","Epoch 15/20\n","171/171 [==============================] - 3s 15ms/step - loss: 0.0343 - val_loss: 0.0152\n","Epoch 16/20\n","171/171 [==============================] - 3s 15ms/step - loss: 0.0343 - val_loss: 0.0153\n","Epoch 17/20\n","171/171 [==============================] - 3s 16ms/step - loss: 0.0343 - val_loss: 0.0153\n","Epoch 18/20\n","171/171 [==============================] - 3s 15ms/step - loss: 0.0343 - val_loss: 0.0153\n","Epoch 19/20\n","171/171 [==============================] - 3s 15ms/step - loss: 0.0342 - val_loss: 0.0153\n","Epoch 20/20\n","171/171 [==============================] - 3s 16ms/step - loss: 0.0342 - val_loss: 0.0153\n","Fitting 23/30 model\n","Epoch 1/20\n","171/171 [==============================] - 5s 28ms/step - loss: 0.0512 - val_loss: 0.0146\n","Epoch 2/20\n","171/171 [==============================] - 4s 23ms/step - loss: 0.0375 - val_loss: 0.0148\n","Epoch 3/20\n","171/171 [==============================] - 4s 23ms/step - loss: 0.0391 - val_loss: 0.0149\n","Epoch 4/20\n","171/171 [==============================] - 4s 23ms/step - loss: 0.0345 - val_loss: 0.0153\n","Epoch 5/20\n","171/171 [==============================] - 4s 23ms/step - loss: 0.0370 - val_loss: 0.0148\n","Epoch 6/20\n","171/171 [==============================] - 4s 22ms/step - loss: 0.0382 - val_loss: 0.0148\n","Epoch 7/20\n","171/171 [==============================] - 4s 24ms/step - loss: 0.0367 - val_loss: 0.0149\n","Epoch 8/20\n","171/171 [==============================] - 4s 22ms/step - loss: 0.0356 - val_loss: 0.0150\n","Epoch 9/20\n","171/171 [==============================] - 4s 23ms/step - loss: 0.0377 - val_loss: 0.0149\n","Epoch 10/20\n","171/171 [==============================] - 4s 22ms/step - loss: 0.0375 - val_loss: 0.0153\n","Epoch 11/20\n","171/171 [==============================] - 4s 22ms/step - loss: 0.0370 - val_loss: 0.0151\n","Epoch 12/20\n","171/171 [==============================] - 4s 23ms/step - loss: 0.0363 - val_loss: 0.0151\n","Epoch 13/20\n","171/171 [==============================] - 4s 22ms/step - loss: 0.0383 - val_loss: 0.0148\n","Epoch 14/20\n","171/171 [==============================] - 4s 21ms/step - loss: 0.0389 - val_loss: 0.0150\n","Epoch 15/20\n","171/171 [==============================] - 4s 22ms/step - loss: 0.0366 - val_loss: 0.0153\n","Epoch 16/20\n","171/171 [==============================] - 4s 22ms/step - loss: 0.0371 - val_loss: 0.0154\n","Epoch 17/20\n","171/171 [==============================] - 4s 22ms/step - loss: 0.0369 - val_loss: 0.0155\n","Epoch 18/20\n","171/171 [==============================] - 4s 22ms/step - loss: 0.0358 - val_loss: 0.0156\n","Epoch 19/20\n","171/171 [==============================] - 4s 22ms/step - loss: 0.0361 - val_loss: 0.0151\n","Epoch 20/20\n","171/171 [==============================] - 4s 21ms/step - loss: 0.0351 - val_loss: 0.0151\n","Fitting 24/30 model\n","Epoch 1/20\n","171/171 [==============================] - 5s 28ms/step - loss: 0.0447 - val_loss: 0.0149\n","Epoch 2/20\n","171/171 [==============================] - 4s 22ms/step - loss: 0.0346 - val_loss: 0.0149\n","Epoch 3/20\n","171/171 [==============================] - 4s 23ms/step - loss: 0.0346 - val_loss: 0.0149\n","Epoch 4/20\n","171/171 [==============================] - 5s 30ms/step - loss: 0.0347 - val_loss: 0.0149\n","Epoch 5/20\n","171/171 [==============================] - 6s 36ms/step - loss: 0.0347 - val_loss: 0.0150\n","Epoch 6/20\n","171/171 [==============================] - 4s 23ms/step - loss: 0.0347 - val_loss: 0.0150\n","Epoch 7/20\n","171/171 [==============================] - 4s 22ms/step - loss: 0.0347 - val_loss: 0.0150\n","Epoch 8/20\n","171/171 [==============================] - 4s 21ms/step - loss: 0.0346 - val_loss: 0.0150\n","Epoch 9/20\n","171/171 [==============================] - 4s 21ms/step - loss: 0.0346 - val_loss: 0.0151\n","Epoch 10/20\n","171/171 [==============================] - 4s 22ms/step - loss: 0.0346 - val_loss: 0.0151\n","Epoch 11/20\n","171/171 [==============================] - 4s 22ms/step - loss: 0.0346 - val_loss: 0.0151\n","Epoch 12/20\n","171/171 [==============================] - 4s 22ms/step - loss: 0.0346 - val_loss: 0.0151\n","Epoch 13/20\n","171/171 [==============================] - 4s 22ms/step - loss: 0.0346 - val_loss: 0.0152\n","Epoch 14/20\n","171/171 [==============================] - 4s 22ms/step - loss: 0.0346 - val_loss: 0.0152\n","Epoch 15/20\n","171/171 [==============================] - 4s 23ms/step - loss: 0.0346 - val_loss: 0.0152\n","Epoch 16/20\n","171/171 [==============================] - 4s 21ms/step - loss: 0.0346 - val_loss: 0.0152\n","Epoch 17/20\n","171/171 [==============================] - 4s 22ms/step - loss: 0.0346 - val_loss: 0.0152\n","Epoch 18/20\n","171/171 [==============================] - 4s 21ms/step - loss: 0.0346 - val_loss: 0.0152\n","Epoch 19/20\n","171/171 [==============================] - 4s 21ms/step - loss: 0.0346 - val_loss: 0.0153\n","Epoch 20/20\n","171/171 [==============================] - 4s 22ms/step - loss: 0.0346 - val_loss: 0.0153\n","Fitting 25/30 model\n","Epoch 1/20\n","139/139 [==============================] - 3s 18ms/step - loss: 0.0544 - val_loss: 0.0194\n","Epoch 2/20\n","139/139 [==============================] - 2s 17ms/step - loss: 0.0421 - val_loss: 0.0158\n","Epoch 3/20\n","139/139 [==============================] - 2s 17ms/step - loss: 0.0360 - val_loss: 0.0155\n","Epoch 4/20\n","139/139 [==============================] - 2s 17ms/step - loss: 0.0353 - val_loss: 0.0156\n","Epoch 5/20\n","139/139 [==============================] - 2s 16ms/step - loss: 0.0345 - val_loss: 0.0156\n","Epoch 6/20\n","139/139 [==============================] - 2s 16ms/step - loss: 0.0353 - val_loss: 0.0157\n","Epoch 7/20\n","139/139 [==============================] - 2s 17ms/step - loss: 0.0352 - val_loss: 0.0157\n","Epoch 8/20\n","139/139 [==============================] - 2s 16ms/step - loss: 0.0346 - val_loss: 0.0156\n","Epoch 9/20\n","139/139 [==============================] - 2s 17ms/step - loss: 0.0357 - val_loss: 0.0156\n","Epoch 10/20\n","139/139 [==============================] - 2s 17ms/step - loss: 0.0364 - val_loss: 0.0156\n","Epoch 11/20\n","139/139 [==============================] - 2s 17ms/step - loss: 0.0352 - val_loss: 0.0157\n","Epoch 12/20\n","139/139 [==============================] - 2s 17ms/step - loss: 0.0346 - val_loss: 0.0157\n","Epoch 13/20\n","139/139 [==============================] - 2s 17ms/step - loss: 0.0350 - val_loss: 0.0157\n","Epoch 14/20\n","139/139 [==============================] - 2s 17ms/step - loss: 0.0341 - val_loss: 0.0158\n","Epoch 15/20\n","139/139 [==============================] - 2s 16ms/step - loss: 0.0367 - val_loss: 0.0157\n","Epoch 16/20\n","139/139 [==============================] - 2s 16ms/step - loss: 0.0331 - val_loss: 0.0157\n","Epoch 17/20\n","139/139 [==============================] - 2s 17ms/step - loss: 0.0343 - val_loss: 0.0156\n","Epoch 18/20\n","139/139 [==============================] - 2s 16ms/step - loss: 0.0351 - val_loss: 0.0157\n","Epoch 19/20\n","139/139 [==============================] - 2s 16ms/step - loss: 0.0346 - val_loss: 0.0158\n","Epoch 20/20\n","139/139 [==============================] - 2s 17ms/step - loss: 0.0351 - val_loss: 0.0157\n","Fitting 26/30 model\n","Epoch 1/20\n","139/139 [==============================] - 3s 19ms/step - loss: 0.0564 - val_loss: 0.0190\n","Epoch 2/20\n","139/139 [==============================] - 2s 16ms/step - loss: 0.0395 - val_loss: 0.0158\n","Epoch 3/20\n","139/139 [==============================] - 2s 17ms/step - loss: 0.0346 - val_loss: 0.0159\n","Epoch 4/20\n","139/139 [==============================] - 2s 16ms/step - loss: 0.0343 - val_loss: 0.0159\n","Epoch 5/20\n","139/139 [==============================] - 2s 17ms/step - loss: 0.0342 - val_loss: 0.0159\n","Epoch 6/20\n","139/139 [==============================] - 2s 17ms/step - loss: 0.0341 - val_loss: 0.0159\n","Epoch 7/20\n","139/139 [==============================] - 2s 17ms/step - loss: 0.0341 - val_loss: 0.0159\n","Epoch 8/20\n","139/139 [==============================] - 2s 17ms/step - loss: 0.0341 - val_loss: 0.0159\n","Epoch 9/20\n","139/139 [==============================] - 2s 17ms/step - loss: 0.0341 - val_loss: 0.0159\n","Epoch 10/20\n","139/139 [==============================] - 2s 17ms/step - loss: 0.0340 - val_loss: 0.0159\n","Epoch 11/20\n","139/139 [==============================] - 2s 17ms/step - loss: 0.0340 - val_loss: 0.0159\n","Epoch 12/20\n","139/139 [==============================] - 2s 17ms/step - loss: 0.0340 - val_loss: 0.0159\n","Epoch 13/20\n","139/139 [==============================] - 2s 17ms/step - loss: 0.0340 - val_loss: 0.0159\n","Epoch 14/20\n","139/139 [==============================] - 2s 17ms/step - loss: 0.0340 - val_loss: 0.0159\n","Epoch 15/20\n","139/139 [==============================] - 2s 17ms/step - loss: 0.0340 - val_loss: 0.0159\n","Epoch 16/20\n","139/139 [==============================] - 2s 17ms/step - loss: 0.0339 - val_loss: 0.0159\n","Epoch 17/20\n","139/139 [==============================] - 2s 16ms/step - loss: 0.0339 - val_loss: 0.0159\n","Epoch 18/20\n","139/139 [==============================] - 2s 16ms/step - loss: 0.0339 - val_loss: 0.0159\n","Epoch 19/20\n","139/139 [==============================] - 2s 17ms/step - loss: 0.0339 - val_loss: 0.0159\n","Epoch 20/20\n","139/139 [==============================] - 2s 16ms/step - loss: 0.0339 - val_loss: 0.0159\n","Fitting 27/30 model\n","Epoch 1/20\n","139/139 [==============================] - 5s 38ms/step - loss: 0.0569 - val_loss: 0.0180\n","Epoch 2/20\n","139/139 [==============================] - 5s 35ms/step - loss: 0.0387 - val_loss: 0.0155\n","Epoch 3/20\n","139/139 [==============================] - 5s 34ms/step - loss: 0.0367 - val_loss: 0.0156\n","Epoch 4/20\n","139/139 [==============================] - 5s 34ms/step - loss: 0.0345 - val_loss: 0.0156\n","Epoch 5/20\n","139/139 [==============================] - 5s 34ms/step - loss: 0.0347 - val_loss: 0.0156\n","Epoch 6/20\n","139/139 [==============================] - 5s 34ms/step - loss: 0.0364 - val_loss: 0.0156\n","Epoch 7/20\n","139/139 [==============================] - 5s 33ms/step - loss: 0.0362 - val_loss: 0.0156\n","Epoch 8/20\n","139/139 [==============================] - 5s 34ms/step - loss: 0.0356 - val_loss: 0.0156\n","Epoch 9/20\n","139/139 [==============================] - 5s 32ms/step - loss: 0.0340 - val_loss: 0.0157\n","Epoch 10/20\n","139/139 [==============================] - 5s 33ms/step - loss: 0.0339 - val_loss: 0.0157\n","Epoch 11/20\n","139/139 [==============================] - 5s 33ms/step - loss: 0.0355 - val_loss: 0.0157\n","Epoch 12/20\n","139/139 [==============================] - 5s 33ms/step - loss: 0.0346 - val_loss: 0.0156\n","Epoch 13/20\n","139/139 [==============================] - 4s 32ms/step - loss: 0.0361 - val_loss: 0.0156\n","Epoch 14/20\n","139/139 [==============================] - 5s 33ms/step - loss: 0.0365 - val_loss: 0.0156\n","Epoch 15/20\n","139/139 [==============================] - 4s 32ms/step - loss: 0.0359 - val_loss: 0.0157\n","Epoch 16/20\n","139/139 [==============================] - 5s 33ms/step - loss: 0.0380 - val_loss: 0.0156\n","Epoch 17/20\n","139/139 [==============================] - 5s 33ms/step - loss: 0.0341 - val_loss: 0.0157\n","Epoch 18/20\n","139/139 [==============================] - 5s 33ms/step - loss: 0.0343 - val_loss: 0.0157\n","Epoch 19/20\n","139/139 [==============================] - 5s 32ms/step - loss: 0.0346 - val_loss: 0.0157\n","Epoch 20/20\n","139/139 [==============================] - 4s 32ms/step - loss: 0.0347 - val_loss: 0.0157\n","Fitting 28/30 model\n","Epoch 1/20\n","139/139 [==============================] - 5s 37ms/step - loss: 0.0517 - val_loss: 0.0165\n","Epoch 2/20\n","139/139 [==============================] - 5s 34ms/step - loss: 0.0363 - val_loss: 0.0157\n","Epoch 3/20\n","139/139 [==============================] - 5s 33ms/step - loss: 0.0344 - val_loss: 0.0157\n","Epoch 4/20\n","139/139 [==============================] - 5s 33ms/step - loss: 0.0343 - val_loss: 0.0157\n","Epoch 5/20\n","139/139 [==============================] - 5s 33ms/step - loss: 0.0343 - val_loss: 0.0157\n","Epoch 6/20\n","139/139 [==============================] - 5s 33ms/step - loss: 0.0342 - val_loss: 0.0158\n","Epoch 7/20\n","139/139 [==============================] - 5s 34ms/step - loss: 0.0342 - val_loss: 0.0158\n","Epoch 8/20\n","139/139 [==============================] - 5s 33ms/step - loss: 0.0342 - val_loss: 0.0158\n","Epoch 9/20\n","139/139 [==============================] - 5s 33ms/step - loss: 0.0342 - val_loss: 0.0158\n","Epoch 10/20\n","139/139 [==============================] - 5s 33ms/step - loss: 0.0341 - val_loss: 0.0158\n","Epoch 11/20\n","139/139 [==============================] - 5s 34ms/step - loss: 0.0341 - val_loss: 0.0158\n","Epoch 12/20\n","139/139 [==============================] - 5s 33ms/step - loss: 0.0341 - val_loss: 0.0158\n","Epoch 13/20\n","139/139 [==============================] - 4s 32ms/step - loss: 0.0341 - val_loss: 0.0158\n","Epoch 14/20\n","139/139 [==============================] - 5s 33ms/step - loss: 0.0341 - val_loss: 0.0158\n","Epoch 15/20\n","139/139 [==============================] - 5s 34ms/step - loss: 0.0341 - val_loss: 0.0158\n","Epoch 16/20\n","139/139 [==============================] - 5s 33ms/step - loss: 0.0340 - val_loss: 0.0158\n","Epoch 17/20\n","139/139 [==============================] - 5s 33ms/step - loss: 0.0340 - val_loss: 0.0158\n","Epoch 18/20\n","139/139 [==============================] - 5s 33ms/step - loss: 0.0340 - val_loss: 0.0157\n","Epoch 19/20\n","139/139 [==============================] - 5s 34ms/step - loss: 0.0340 - val_loss: 0.0157\n","Epoch 20/20\n","139/139 [==============================] - 5s 34ms/step - loss: 0.0340 - val_loss: 0.0157\n","Fitting 29/30 model\n","Epoch 1/20\n","139/139 [==============================] - 8s 55ms/step - loss: 0.0493 - val_loss: 0.0169\n","Epoch 2/20\n","139/139 [==============================] - 7s 50ms/step - loss: 0.0387 - val_loss: 0.0157\n","Epoch 3/20\n","139/139 [==============================] - 7s 48ms/step - loss: 0.0352 - val_loss: 0.0157\n","Epoch 4/20\n","139/139 [==============================] - 7s 47ms/step - loss: 0.0352 - val_loss: 0.0157\n","Epoch 5/20\n","139/139 [==============================] - 7s 49ms/step - loss: 0.0343 - val_loss: 0.0159\n","Epoch 6/20\n","139/139 [==============================] - 7s 48ms/step - loss: 0.0368 - val_loss: 0.0157\n","Epoch 7/20\n","139/139 [==============================] - 7s 50ms/step - loss: 0.0366 - val_loss: 0.0157\n","Epoch 8/20\n","139/139 [==============================] - 7s 49ms/step - loss: 0.0365 - val_loss: 0.0157\n","Epoch 9/20\n","139/139 [==============================] - 7s 47ms/step - loss: 0.0370 - val_loss: 0.0157\n","Epoch 10/20\n","139/139 [==============================] - 7s 48ms/step - loss: 0.0352 - val_loss: 0.0157\n","Epoch 11/20\n","139/139 [==============================] - 7s 48ms/step - loss: 0.0343 - val_loss: 0.0157\n","Epoch 12/20\n","139/139 [==============================] - 7s 47ms/step - loss: 0.0360 - val_loss: 0.0158\n","Epoch 13/20\n","139/139 [==============================] - 7s 47ms/step - loss: 0.0350 - val_loss: 0.0157\n","Epoch 14/20\n","139/139 [==============================] - 6s 47ms/step - loss: 0.0364 - val_loss: 0.0158\n","Epoch 15/20\n","139/139 [==============================] - 6s 46ms/step - loss: 0.0362 - val_loss: 0.0157\n","Epoch 16/20\n","139/139 [==============================] - 6s 46ms/step - loss: 0.0364 - val_loss: 0.0158\n","Epoch 17/20\n","139/139 [==============================] - 6s 46ms/step - loss: 0.0366 - val_loss: 0.0157\n","Epoch 18/20\n","139/139 [==============================] - 7s 48ms/step - loss: 0.0351 - val_loss: 0.0158\n","Epoch 19/20\n","139/139 [==============================] - 6s 46ms/step - loss: 0.0352 - val_loss: 0.0158\n","Epoch 20/20\n","139/139 [==============================] - 7s 47ms/step - loss: 0.0363 - val_loss: 0.0157\n","Fitting 30/30 model\n","Epoch 1/20\n","139/139 [==============================] - 8s 57ms/step - loss: 0.0484 - val_loss: 0.0158\n","Epoch 2/20\n","139/139 [==============================] - 7s 49ms/step - loss: 0.0351 - val_loss: 0.0156\n","Epoch 3/20\n","139/139 [==============================] - 7s 49ms/step - loss: 0.0345 - val_loss: 0.0156\n","Epoch 4/20\n","139/139 [==============================] - 7s 50ms/step - loss: 0.0345 - val_loss: 0.0156\n","Epoch 5/20\n","139/139 [==============================] - 7s 48ms/step - loss: 0.0345 - val_loss: 0.0156\n","Epoch 6/20\n","139/139 [==============================] - 7s 48ms/step - loss: 0.0344 - val_loss: 0.0156\n","Epoch 7/20\n","139/139 [==============================] - 6s 46ms/step - loss: 0.0344 - val_loss: 0.0156\n","Epoch 8/20\n","139/139 [==============================] - 7s 48ms/step - loss: 0.0344 - val_loss: 0.0156\n","Epoch 9/20\n","139/139 [==============================] - 7s 48ms/step - loss: 0.0344 - val_loss: 0.0156\n","Epoch 10/20\n","139/139 [==============================] - 7s 47ms/step - loss: 0.0344 - val_loss: 0.0156\n","Epoch 11/20\n","139/139 [==============================] - 6s 47ms/step - loss: 0.0344 - val_loss: 0.0156\n","Epoch 12/20\n","139/139 [==============================] - 7s 47ms/step - loss: 0.0343 - val_loss: 0.0156\n","Epoch 13/20\n","139/139 [==============================] - 7s 48ms/step - loss: 0.0343 - val_loss: 0.0156\n","Epoch 14/20\n","139/139 [==============================] - 7s 48ms/step - loss: 0.0343 - val_loss: 0.0156\n","Epoch 15/20\n","139/139 [==============================] - 7s 48ms/step - loss: 0.0343 - val_loss: 0.0156\n","Epoch 16/20\n","139/139 [==============================] - 8s 56ms/step - loss: 0.0343 - val_loss: 0.0157\n","Epoch 17/20\n","139/139 [==============================] - 9s 67ms/step - loss: 0.0343 - val_loss: 0.0157\n","Epoch 18/20\n","139/139 [==============================] - 6s 46ms/step - loss: 0.0343 - val_loss: 0.0157\n","Epoch 19/20\n","139/139 [==============================] - 7s 47ms/step - loss: 0.0343 - val_loss: 0.0157\n","Epoch 20/20\n","139/139 [==============================] - 7s 47ms/step - loss: 0.0343 - val_loss: 0.0157\n"],"name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-204-1c44136fed41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Risk_Score = Sub_WeeklyTS['Risk_Score'].to_numpy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRisk_Score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparams_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"]}]},{"cell_type":"code","metadata":{"id":"uKOD5x-Y63VK"},"source":["# model with lowest MSE\n","params = {'input_size': 10,\n","          'hidden_units':100,\n","          'dropout': True,\n","          'learning_rate':4e-5,\n","          'n_ahead':5,\n","          'val_split': 0.2,\n","          'epochs':20,\n","          'verbose':False,\n","          'plot':False}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JeEwZpFluu8I","executionInfo":{"status":"ok","timestamp":1605062917750,"user_tz":360,"elapsed":17481,"user":{"displayName":"Dushyant Singh Khinchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhVgT4J-0PZxTmXsjDZlk4iTD5UX35a5leV156K=s64","userId":"14979082350769729724"}},"outputId":"e742fb10-0660-4f8f-f412-29ee64f9ee20","colab":{"base_uri":"https://localhost:8080/"}},"source":["model_, mse, hist, best_prediction, yt  = FitEvaluate(Risk_Score,params)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/20\n","187/187 [==============================] - 1s 6ms/step - loss: 0.0738 - val_loss: 0.0174\n","Epoch 2/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0481 - val_loss: 0.0150\n","Epoch 3/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0376 - val_loss: 0.0157\n","Epoch 4/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0397 - val_loss: 0.0157\n","Epoch 5/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0390 - val_loss: 0.0159\n","Epoch 6/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0392 - val_loss: 0.0158\n","Epoch 7/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0379 - val_loss: 0.0159\n","Epoch 8/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0383 - val_loss: 0.0160\n","Epoch 9/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0378 - val_loss: 0.0160\n","Epoch 10/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0396 - val_loss: 0.0160\n","Epoch 11/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0371 - val_loss: 0.0163\n","Epoch 12/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0381 - val_loss: 0.0162\n","Epoch 13/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0383 - val_loss: 0.0161\n","Epoch 14/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0394 - val_loss: 0.0160\n","Epoch 15/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0386 - val_loss: 0.0161\n","Epoch 16/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0381 - val_loss: 0.0158\n","Epoch 17/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0390 - val_loss: 0.0159\n","Epoch 18/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0384 - val_loss: 0.0160\n","Epoch 19/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0375 - val_loss: 0.0161\n","Epoch 20/20\n","187/187 [==============================] - 1s 4ms/step - loss: 0.0381 - val_loss: 0.0158\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vxo744b7y2nB"},"source":["ViewLoss(hist)\n","view_predictions(Risk_Score,best_prediction,yt,'Actual vs Forecast')\n","print('mean-squared-error: {}'.format(mse))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"67LGPOU1RF55"},"source":["## Transfer Learning"]},{"cell_type":"code","metadata":{"id":"mWqhFGkYsMb6"},"source":["Sub_WeeklyTS = WeeklyTS[WeeklyTS['repoID']==163683035.0].sort_values(by=['Date'], ascending=True)\n","Sub_WeeklyTS.index=Sub_WeeklyTS['Date']\n","Sub_WeeklyTS.drop(['Date','repoID'], axis=1, inplace=True)\n","Risk_Score = Sub_WeeklyTS.Risk_Score.to_numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bY-y2-1G3nVh","executionInfo":{"status":"ok","timestamp":1605232610988,"user_tz":360,"elapsed":16038,"user":{"displayName":"Dushyant Singh Khinchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhVgT4J-0PZxTmXsjDZlk4iTD5UX35a5leV156K=s64","userId":"14979082350769729724"}},"outputId":"a907d14a-808a-41d7-acae-d9b95bf99edc","colab":{"base_uri":"https://localhost:8080/"}},"source":["\n","# model_ = keras.models.load_model('Transfer_learning_model')\n","\n","\n","# model with lowest MSE\n","params = {'input_size': 10,\n","          'hidden_units':100,\n","          'dropout': True,\n","          'learning_rate':4e-5,\n","          'n_ahead':5,\n","          'val_split': 0.2,\n","          'epochs':20,\n","          'verbose':False,\n","          'plot':False}\n","model_, mse, hist, best_prediction, yt  = FitEvaluate(Risk_Score,params)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/20\n","187/187 [==============================] - 1s 5ms/step - loss: 0.0000e+00 - val_loss: 0.5181\n","Epoch 2/20\n","187/187 [==============================] - 1s 4ms/step - loss: 6.2822e-07 - val_loss: 0.5351\n","Epoch 3/20\n","187/187 [==============================] - 1s 3ms/step - loss: 4.9163e-06 - val_loss: 0.5132\n","Epoch 4/20\n","187/187 [==============================] - 1s 3ms/step - loss: 3.1239e-07 - val_loss: 0.5084\n","Epoch 5/20\n","187/187 [==============================] - 1s 4ms/step - loss: 1.2966e-07 - val_loss: 0.5146\n","Epoch 6/20\n","187/187 [==============================] - 1s 3ms/step - loss: 1.9476e-07 - val_loss: 0.5171\n","Epoch 7/20\n","187/187 [==============================] - 1s 3ms/step - loss: 1.4500e-06 - val_loss: 0.5001\n","Epoch 8/20\n","187/187 [==============================] - 1s 3ms/step - loss: 2.0325e-06 - val_loss: 0.5233\n","Epoch 9/20\n","187/187 [==============================] - 1s 3ms/step - loss: 7.1029e-07 - val_loss: 0.5101\n","Epoch 10/20\n","187/187 [==============================] - 1s 3ms/step - loss: 1.2744e-07 - val_loss: 0.5147\n","Epoch 11/20\n","187/187 [==============================] - 1s 3ms/step - loss: 8.1529e-07 - val_loss: 0.5314\n","Epoch 12/20\n","187/187 [==============================] - 1s 4ms/step - loss: 1.5133e-06 - val_loss: 0.5136\n","Epoch 13/20\n","187/187 [==============================] - 1s 4ms/step - loss: 1.0485e-06 - val_loss: 0.5005\n","Epoch 14/20\n","187/187 [==============================] - 1s 3ms/step - loss: 9.2527e-08 - val_loss: 0.4997\n","Epoch 15/20\n","187/187 [==============================] - 1s 4ms/step - loss: 2.6223e-08 - val_loss: 0.5009\n","Epoch 16/20\n","187/187 [==============================] - 1s 4ms/step - loss: 8.8399e-07 - val_loss: 0.4847\n","Epoch 17/20\n","187/187 [==============================] - 1s 3ms/step - loss: 2.1412e-06 - val_loss: 0.5075\n","Epoch 18/20\n","187/187 [==============================] - 1s 4ms/step - loss: 2.4324e-06 - val_loss: 0.4880\n","Epoch 19/20\n","187/187 [==============================] - 1s 4ms/step - loss: 1.2979e-06 - val_loss: 0.5091\n","Epoch 20/20\n","187/187 [==============================] - 1s 4ms/step - loss: 2.4997e-07 - val_loss: 0.5009\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uXmCvyzsEmLo","executionInfo":{"status":"ok","timestamp":1605247650837,"user_tz":360,"elapsed":623131,"user":{"displayName":"Dushyant Singh Khinchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhVgT4J-0PZxTmXsjDZlk4iTD5UX35a5leV156K=s64","userId":"14979082350769729724"}},"outputId":"3008f172-9e86-4823-dd06-7d373a4e7003","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1Iw5i4icTFPVmgoxXoA8PrE6HNSTY0020"}},"source":["for r in range(0,len(repo_ids)):\n","        print('******Training on Repo ID: {} ********'.format(repo_ids[r]))\n","\n","        Sub_WeeklyTS = WeeklyTS[WeeklyTS['repoID']==repo_ids[r]].sort_values(by=['Date'])\n","        Sub_WeeklyTS.index=Sub_WeeklyTS['Date']\n","        Sub_WeeklyTS.drop('Date',axis=1,inplace=True)\n","        Sub_WeeklyTS.drop(['repoID'], axis=1, inplace=True)\n","        Risk_Score = Sub_WeeklyTS.Risk_Score.to_numpy()\n","\n","\n","        model_ = TransferLearning( Risk_Score,params,model=model_)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"P8YBGNmzU14A"},"source":["# serialize model to JSON\n","model_json =model_.to_json()\n","\n","with open(\"Transfer_learning_model_BiggerDS_Imputation.json\", \"w\") as json_file:\n","    json_file.write(model_json)\n","\n","# serialize weights to HDF5\n","model_.save('Transfer_learning_model_BiggerDS_Imputation.h5')\n","\n","model_.save_weights(\"Transfer_learning_model_BiggerDS_Imputation.h5\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k5jcsSOxRFOK"},"source":["model_.get_weights()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-rHHSG-czAGG"},"source":["# Testing model"]},{"cell_type":"code","metadata":{"id":"BMWpVi_T13lh"},"source":["def Test_FitEvaluate(time_series,params,model_):\n","    '''\n","    Calls the pipeline to fit an LSTM model to the \n","    given time series\n","    \n","    Args:\n","        time_series: the time series of interest\n","        params: a dictionary specifying parameters\n","                {input_size, hidden_units, dropout,\n","                learning_rate, n_ahead, val_split, \n","                epochs, verbose, plot}\n","    Returns:\n","        model: keras sequential model      \n","        mse: mean squared error of the prediction\n","        history: training and validation loss history\n","        \n","    '''   \n","    \n","    for k in params.keys():\n","        globals()[k] = params[k]\n","    \n","    \n","    scaled_series, scaler = preprocessing(time_series)\n","    series, y_test, n_test = getSeries(scaled_series,0.8)\n","    X_train,y_train,X_test = getInputOutput(series,input_size)\n","    \n","    # show only n_ahead number of actual values\n","    y_test = y_test[np.arange(n_ahead)]\n","\n","    new_model, predictions, history = FitForecast(X_train,y_train,X_test,n_ahead,\n","                                        input_size,hidden_units,dropout, val_split,\n","                                        learning_rate,epochs,trained_model=model_)\n","    \n","    # rescaling\n","    series = inverse_transform(series, scaler)\n","    y_test = inverse_transform(y_test, scaler)\n","    predictions = inverse_transform(predictions, scaler)\n","    \n","    mse = mean_squared_error(y_true=y_test,y_pred=predictions)\n","    \n","    print('MSE is {}'.format(round(mse,4)))\n","        \n","\n","    ViewLoss(history)\n","    view_predictions(series,predictions,y_test,'Actual vs Forecast')\n","    return new_model, mse, history, predictions, y_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wJlPH77ZsAGu"},"source":["from tensorflow import keras\n","\n","model_ = keras.models.load_model('Transfer_learning_model')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XCAK46q6Eeb8"},"source":["Sub_WeeklyTS = WeeklyTS[WeeklyTS['repoID']==20873044].sort_values(by=['Date'])\n","Sub_WeeklyTS.index=Sub_WeeklyTS['Date']\n","Sub_WeeklyTS.drop('Date',axis=1,inplace=True)\n","idx = pd.date_range('2015-01-01', today, freq='W-SUN')\n","Sub_WeeklyTS.index = pd.DatetimeIndex(Sub_WeeklyTS.index)\n","Sub_WeeklyTS = Sub_WeeklyTS.reindex(idx, fill_value=0)\n","Sub_WeeklyTS.drop(['repoID'], axis=1, inplace=True)\n","Risk_Score = Sub_WeeklyTS.Risk_Score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2kbaIrTJfCh-"},"source":["new_model_, mse, history, predictions, y_test = Test_FitEvaluate(Risk_Score, params, model_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XHWden8FirN-"},"source":[""],"execution_count":null,"outputs":[]}]}